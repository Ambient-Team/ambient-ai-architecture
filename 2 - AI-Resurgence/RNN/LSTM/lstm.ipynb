{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68ff7353",
   "metadata": {},
   "source": [
    "# ðŸ§  LSTM for Deep Temporal Forecasting\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates **Long Short-Term Memory (LSTM)** networks for advanced time-series forecasting, specifically neural energy demand prediction for smart grids. We'll show how LSTMs overcome fundamental RNN limitations to capture long-range temporal dependencies.\n",
    "\n",
    "### The Vanishing Gradient Problem in Standard RNNs\n",
    "\n",
    "Standard Recurrent Neural Networks (RNNs) suffer from a critical limitation when processing long sequences:\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "During backpropagation through time, gradients are multiplied at each timestep:\n",
    "$$\\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L}{\\partial h_{t+1}} \\cdot \\frac{\\partial h_{t+1}}{\\partial h_t}$$\n",
    "\n",
    "When gradients are small (<0.1), they shrink exponentially with sequence length:\n",
    "$$\\text{Gradient at time } t = \\text{Gradient at time } t+1 \\times 0.1^n$$\n",
    "\n",
    "**Consequence**: After 10-20 timesteps, gradients become effectively zero, preventing learning of long-range dependencies.\n",
    "\n",
    "### LSTM Solution: Gate Mechanism\n",
    "\n",
    "LSTMs introduce **three gating mechanisms** to control information flow:\n",
    "\n",
    "**1. Forget Gate** ($f_t$): Decides which information to discard\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "**2. Input Gate** ($i_t$): Decides which new information to add\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "\n",
    "**3. Output Gate** ($o_t$): Decides what to output\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "\n",
    "**Cell State Update** (allows gradient flow):\n",
    "$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)$$\n",
    "\n",
    "**Why This Works**: The cell state ($C_t$) uses **additive** operations (not multiplicative) and **gated** outputs. This creates \"highways\" for gradients to flow, maintaining strength over 100+ timesteps.\n",
    "\n",
    "### Use Case: Smart Grid Neural Forecasting\n",
    "We predict electricity load using LSTM to:\n",
    "- Capture seasonal patterns (daily, weekly, yearly cycles)\n",
    "- Learn trend shifts and regime changes\n",
    "- Respond to extreme weather events with context\n",
    "- Enable 24-48 hour forecasts for grid balancing\n",
    "\n",
    "This notebook evolves from our previous XGBoost module, showcasing the progression from classical ensemble methods to deep learning for temporal data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba20b1c2",
   "metadata": {},
   "source": [
    "## Notebook Structure\n",
    "1. **Import Required Libraries** - TensorFlow/Keras, NumPy, Pandas, Matplotlib\n",
    "2. **Generate Synthetic Time-Series Data** - 10,000 steps with seasons, trends, weather events\n",
    "3. **Exploratory Time-Series Analysis** - Visualize patterns and autocorrelation\n",
    "4. **Data Normalization** - MinMaxScaler for LSTM sensitivity\n",
    "5. **Sliding Window Preprocessing** - Transform sequences into (samples, timesteps, features)\n",
    "6. **LSTM Architecture Design** - Stacked LSTMs with Dropout and Dense output\n",
    "7. **Model Training & Validation** - Track loss curves and convergence\n",
    "8. **Visualization: Training Dynamics** - Loss curves and learning progression\n",
    "9. **Forecast Generation** - Predict future load values\n",
    "10. **Performance Evaluation** - RMSE, MAE, and Sequence Accuracy\n",
    "11. **XGBoost vs. LSTM Comparison** - Trade-offs in interpretability vs. raw power\n",
    "12. **Model Deployment** - Save in SavedModel format for TensorFlow Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f16e1",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8f860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b61ed79",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Time-Series Data\n",
    "\n",
    "Generate 10,000 timesteps of electricity load with realistic patterns:\n",
    "- **Seasonal Cycle**: Daily (24-hour) and weekly patterns\n",
    "- **Trend**: Gradual increase over time (growth in demand)\n",
    "- **Extreme Weather Events**: Sudden spikes in demand (heat waves, cold snaps)\n",
    "- **Noise**: Random variations simulating sensor noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Synthetic Time-Series Data (10,000 timesteps)\n",
    "n_timesteps = 10000\n",
    "\n",
    "# Time indices\n",
    "t = np.arange(n_timesteps)\n",
    "\n",
    "# Base load (average demand in MW)\n",
    "base_load = 3000\n",
    "\n",
    "# Component 1: Daily seasonal pattern (24-hour cycle)\n",
    "daily_cycle = 400 * np.sin(2 * np.pi * t / 24)\n",
    "\n",
    "# Component 2: Weekly seasonal pattern (7-day cycle)\n",
    "weekly_cycle = 200 * np.sin(2 * np.pi * t / (24 * 7))\n",
    "\n",
    "# Component 3: Trend (gradual increase in demand)\n",
    "trend = 0.2 * t\n",
    "\n",
    "# Component 4: Extreme weather events (heat waves, cold snaps)\n",
    "# Randomly introduce demand spikes\n",
    "extreme_events = np.zeros(n_timesteps)\n",
    "event_indices = np.random.choice(n_timesteps, size=50, replace=False)\n",
    "for idx in event_indices:\n",
    "    # Create localized demand spike (3-day event window)\n",
    "    window = slice(idx, min(idx + 72, n_timesteps))\n",
    "    extreme_events[window] += 500 * np.exp(-((np.arange(min(72, n_timesteps - idx)) ** 2) / 500))\n",
    "\n",
    "# Component 5: Random noise\n",
    "noise = np.random.normal(0, 100, n_timesteps)\n",
    "\n",
    "# Combine all components\n",
    "load = base_load + daily_cycle + weekly_cycle + trend + extreme_events + noise\n",
    "\n",
    "# Ensure positive load values\n",
    "load = np.maximum(load, 500)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'timestep': t,\n",
    "    'load_mw': load,\n",
    "    'hour': t % 24,\n",
    "    'day_of_week': (t // 24) % 7\n",
    "})\n",
    "\n",
    "print(\"Synthetic Time-Series Data Generated\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total Timesteps: {n_timesteps}\")\n",
    "print(f\"Time Period: ~{n_timesteps // 24} days, ~{n_timesteps // (24*7):.1f} weeks\")\n",
    "print(f\"\\nLoad Statistics (MW):\")\n",
    "print(f\"  Mean: {load.mean():.1f}\")\n",
    "print(f\"  Std Dev: {load.std():.1f}\")\n",
    "print(f\"  Min: {load.min():.1f}\")\n",
    "print(f\"  Max: {load.max():.1f}\")\n",
    "print(f\"\\nFirst 10 timesteps:\")\n",
    "print(df.head(10))\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aedfece",
   "metadata": {},
   "source": [
    "## 3. Exploratory Time-Series Analysis\n",
    "\n",
    "Visualize temporal patterns and autocorrelation to understand what an LSTM must learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab62c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Time-Series Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Full time-series (entire 10,000 steps)\n",
    "axes[0, 0].plot(load, color='steelblue', linewidth=0.8)\n",
    "axes[0, 0].set_xlabel('Timestep (hours)', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Load (MW)', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_title('Full Time-Series: 10,000 Hours of Electricity Load', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Zoomed view (first 500 timesteps)\n",
    "axes[0, 1].plot(load[:500], color='darkgreen', linewidth=1.2)\n",
    "axes[0, 1].set_xlabel('Timestep (hours)', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Load (MW)', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_title('Zoomed View: First 500 Hours (Daily & Weekly Patterns)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: 7-day average (smoothed trend)\n",
    "window_size = 24 * 7  # 7-day moving average\n",
    "smoothed = pd.Series(load).rolling(window=window_size).mean()\n",
    "axes[1, 0].plot(load, alpha=0.3, color='gray', label='Raw Load')\n",
    "axes[1, 0].plot(smoothed, color='red', linewidth=2.5, label='7-Day Moving Average')\n",
    "axes[1, 0].set_xlabel('Timestep (hours)', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Load (MW)', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_title('Trend Analysis: Smoothed vs. Raw Load', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Autocorrelation (seasonal patterns)\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "autocorrelation_plot(load, ax=axes[1, 1], lags=500)\n",
    "axes[1, 1].set_title('Autocorrelation: Seasonal Dependencies', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Lag (hours)', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTime-Series Characteristics:\")\n",
    "print(f\"  Daily Periodicity: Clear 24-hour cycle visible\")\n",
    "print(f\"  Weekly Periodicity: Repeating 7-day patterns\")\n",
    "print(f\"  Trend: Gradual increase (demand growth)\")\n",
    "print(f\"  Extreme Events: Sudden spikes from weather events\")\n",
    "print(f\"  Autocorrelation: Strong at lags 24, 48, 72... (multiples of 24)\")\n",
    "print(\"  LSTM Opportunity: Learn these long-range dependencies (50+ lags)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb04f510",
   "metadata": {},
   "source": [
    "## 4. Data Normalization with MinMaxScaler\n",
    "\n",
    "**Why is normalization critical for LSTMs?**\n",
    "\n",
    "LSTMs use activation functions (tanh, sigmoid) that are sensitive to input magnitude:\n",
    "- **tanh range**: -1 to +1 (saturates outside this range)\n",
    "- **sigmoid range**: 0 to 1 (becomes flat outside this range)\n",
    "\n",
    "**Without scaling:**\n",
    "- Large raw values (3000 MW) push activations into saturation zones\n",
    "- Gradients become nearly zero (slow learning)\n",
    "- Model struggles to converge\n",
    "\n",
    "**MinMaxScaler solution:**\n",
    "$$x_{\\text{scaled}} = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}$$\n",
    "\n",
    "Transforms all values to [0, 1] range, keeping activations in optimal operating zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80882953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Reshape load data for scaling (scaler expects 2D input)\n",
    "load_reshaped = load.reshape(-1, 1)\n",
    "\n",
    "# Fit scaler and normalize\n",
    "load_scaled = scaler.fit_transform(load_reshaped)\n",
    "load_scaled = load_scaled.flatten()  # Convert back to 1D\n",
    "\n",
    "print(\"Data Normalization Applied with MinMaxScaler\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Original Load Range: [{load.min():.1f}, {load.max():.1f}] MW\")\n",
    "print(f\"Scaled Load Range: [{load_scaled.min():.4f}, {load_scaled.max():.4f}]\")\n",
    "print(f\"\\nScaler Parameters (for inverse transformation):\")\n",
    "print(f\"  Min Value (fitted): {scaler.data_min_[0]:.2f}\")\n",
    "print(f\"  Max Value (fitted): {scaler.data_max_[0]:.2f}\")\n",
    "print(f\"  Scale: {scaler.scale_[0]:.6f}\")\n",
    "\n",
    "# Demonstrate inverse transformation\n",
    "sample_scaled = load_scaled[0:5]\n",
    "sample_original = scaler.inverse_transform(sample_scaled.reshape(-1, 1))\n",
    "print(f\"\\nVerification (Inverse Transform):\")\n",
    "print(f\"  Scaled: {sample_scaled}\")\n",
    "print(f\"  Original: {sample_original.flatten()}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c9618",
   "metadata": {},
   "source": [
    "## 5. Sliding Window Preprocessing\n",
    "\n",
    "Transform raw 1D time-series into 3D sequences: (samples, timesteps, features)\n",
    "\n",
    "**Key Concept:**\n",
    "- **Timestep (window) size**: Number of historical hours to use for prediction (e.g., 48 hours)\n",
    "- **Prediction target**: Predict the load 1 hour ahead\n",
    "- **Result**: Each sample = 48 hours of history â†’ predict hour 49\n",
    "\n",
    "**Example:**\n",
    "- Timesteps 0-47 (48 hours) â†’ Predict timestep 48\n",
    "- Timesteps 1-48 (48 hours) â†’ Predict timestep 49\n",
    "- Timesteps 2-49 (48 hours) â†’ Predict timestep 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sliding window function\n",
    "def create_sliding_window(data, window_size=48):\n",
    "    \"\"\"\n",
    "    Transform 1D time-series into 3D sequences for LSTM.\n",
    "    \n",
    "    Args:\n",
    "        data: 1D numpy array of time-series values\n",
    "        window_size: Number of timesteps in each sample (historical window)\n",
    "    \n",
    "    Returns:\n",
    "        X: (samples, timesteps, features) - input sequences\n",
    "        y: (samples,) - target values (next timestep)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - window_size):\n",
    "        # X: window of past values (48-hour history)\n",
    "        X.append(data[i:i + window_size])\n",
    "        # y: next value to predict (1-hour ahead)\n",
    "        y.append(data[i + window_size])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sliding windows with 48-hour history\n",
    "window_size = 48  # 2 days of history to predict next hour\n",
    "X, y = create_sliding_window(load_scaled, window_size=window_size)\n",
    "\n",
    "print(\"Sliding Window Preprocessing\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Window Size (Historical Data): {window_size} hours (2 days)\")\n",
    "print(f\"Total Sequences Created: {X.shape[0]}\")\n",
    "print(f\"\\nX Shape: {X.shape} â†’ (samples, timesteps, features)\")\n",
    "print(f\"  - {X.shape[0]} samples\")\n",
    "print(f\"  - {X.shape[1]} timesteps (hours of history)\")\n",
    "print(f\"  - {X.shape[2] if len(X.shape) > 2 else 1} features (load only)\")\n",
    "print(f\"\\ny Shape: {y.shape} â†’ (samples,) â†’ targets to predict\")\n",
    "\n",
    "# Visualize one sample\n",
    "print(f\"\\nExample Sample (Index 0):\")\n",
    "print(f\"  Input (48 hours): {X[0][:5]}... (first 5 of 48)\")\n",
    "print(f\"  Target (hour 49): {y[0]:.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Split into training (80%) and testing (20%)\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"\\nTrain/Test Split:\")\n",
    "print(f\"  Training: {X_train.shape[0]} samples\")\n",
    "print(f\"  Testing: {X_test.shape[0]} samples\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5541ab3",
   "metadata": {},
   "source": [
    "## 6. LSTM Architecture Design\n",
    "\n",
    "Build a deep LSTM network with:\n",
    "1. **Input Layer**: (batch_size, 48 timesteps, 1 feature)\n",
    "2. **LSTM Layer 1**: 64 units with return_sequences (feeds to next LSTM)\n",
    "3. **LSTM Layer 2**: 32 units (final LSTM layer)\n",
    "4. **Dropout**: 0.2 rate for regularization\n",
    "5. **Dense Output**: 1 unit for predicting next load value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM Model\n",
    "model = Sequential([\n",
    "    # Input layer implicitly defined by input shape\n",
    "    LSTM(\n",
    "        units=64,\n",
    "        activation='relu',\n",
    "        return_sequences=True,  # Output sequence for next LSTM layer\n",
    "        input_shape=(window_size, 1)  # (timesteps, features)\n",
    "    ),\n",
    "    Dropout(0.2),  # Regularization: randomly drop 20% of neurons\n",
    "    \n",
    "    # Second LSTM layer (deeper learning)\n",
    "    LSTM(\n",
    "        units=32,\n",
    "        activation='relu',\n",
    "        return_sequences=False  # Final LSTM outputs single value\n",
    "    ),\n",
    "    Dropout(0.2),  # Another dropout for further regularization\n",
    "    \n",
    "    # Dense output layer\n",
    "    Dense(units=1)  # Single output: next load value\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),  # Adam optimizer with learning rate\n",
    "    loss='mse',  # Mean Squared Error for regression\n",
    "    metrics=['mae']  # Track MAE during training\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"LSTM Model Architecture\")\n",
    "print(\"=\" * 70)\n",
    "model.summary()\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nModel Explanation:\")\n",
    "print(\"  Layer 1 - LSTM (64 units):\")\n",
    "print(\"    - Processes 48 timesteps of historical load\")\n",
    "print(\"    - return_sequences=True passes output to next LSTM layer\")\n",
    "print(\"    - Learns long-range temporal patterns (daily, weekly cycles)\")\n",
    "print(\"\")\n",
    "print(\"  Layer 2 - LSTM (32 units):\")\n",
    "print(\"    - Stacks on first LSTM for deeper feature extraction\")\n",
    "print(\"    - return_sequences=False outputs single value\")\n",
    "print(\"    - Learns complex interactions from Layer 1 features\")\n",
    "print(\"\")\n",
    "print(\"  Dropout (0.2):\")\n",
    "print(\"    - Prevents overfitting by randomly silencing neurons\")\n",
    "print(\"    - Each layer uses different random mask\")\n",
    "print(\"\")\n",
    "print(\"  Dense Output (1 unit):\")\n",
    "print(\"    - Predicts next hour's load (single scalar value)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ed789",
   "metadata": {},
   "source": [
    "## 7. Model Training & Validation\n",
    "\n",
    "Train the LSTM on historical data, with validation monitoring to detect overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49266d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training LSTM Model...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Stop if validation loss doesn't improve for 10 epochs\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,  # Maximum 50 epochs\n",
    "    batch_size=32,  # Process 32 samples at a time\n",
    "    validation_split=0.2,  # Use 20% of training data for validation\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0  # Suppress epoch-by-epoch output\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Training Complete!\")\n",
    "print(f\"  Total Epochs: {len(history.history['loss'])}\")\n",
    "print(f\"  Final Training Loss: {history.history['loss'][-1]:.6f}\")\n",
    "print(f\"  Final Validation Loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "print(f\"  Final Training MAE: {history.history['mae'][-1]:.6f}\")\n",
    "print(f\"  Final Validation MAE: {history.history['val_mae'][-1]:.6f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb829ee9",
   "metadata": {},
   "source": [
    "## 8. Training Dynamics Visualization\n",
    "\n",
    "Visualize learning progress through loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc99124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Loss curves (MSE)\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2, marker='o', markersize=4)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2, marker='s', markersize=4)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Loss (MSE)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Training vs. Validation Loss (Lower is Better)', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')  # Log scale to see early improvements\n",
    "\n",
    "# Plot 2: MAE curves\n",
    "axes[1].plot(history.history['mae'], label='Training MAE', linewidth=2, marker='o', markersize=4)\n",
    "axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2, marker='s', markersize=4)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Mean Absolute Error', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Training vs. Validation MAE', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining Dynamics Interpretation:\")\n",
    "print(\"  âœ“ Decreasing training loss: Model learns patterns from data\")\n",
    "print(\"  âœ“ Decreasing validation loss: Generalizes well to unseen data\")\n",
    "print(\"  âš  Validation loss increases: Potential overfitting (dropout helps)\")\n",
    "print(\"  âœ“ Early stopping: Prevents training beyond optimal point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fed443",
   "metadata": {},
   "source": [
    "## 9. Forecast Generation\n",
    "\n",
    "Generate predictions on test set and inverse-scale back to original units (MW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d849da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set\n",
    "y_pred_scaled = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Inverse transform back to original units (MW)\n",
    "y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "y_pred_original = scaler.inverse_transform(y_pred_scaled).flatten()\n",
    "\n",
    "print(\"Forecast Generation Complete\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Predictions Generated: {len(y_pred_original)}\")\n",
    "print(f\"Time Period: {len(y_pred_original)} hours = {len(y_pred_original) / 24:.1f} days\")\n",
    "print(f\"\\nSample Predictions (first 5):\")\n",
    "for i in range(5):\n",
    "    print(f\"  Hour {i+1}: Actual={y_test_original[i]:.1f} MW, Predicted={y_pred_original[i]:.1f} MW\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "forecast_df = pd.DataFrame({\n",
    "    'actual': y_test_original,\n",
    "    'predicted': y_pred_original,\n",
    "    'error': y_test_original - y_pred_original,\n",
    "    'abs_error': np.abs(y_test_original - y_pred_original)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454aefc8",
   "metadata": {},
   "source": [
    "## 10. Performance Evaluation\n",
    "\n",
    "Evaluate LSTM with regression metrics and sequence accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
    "mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "mape = mean_absolute_percentage_error(y_test_original, y_pred_original)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LSTM PERFORMANCE EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nðŸ“Š STANDARD REGRESSION METRICS\")\n",
    "print(f\"   RMSE (Root Mean Squared Error): {rmse:.2f} MW\")\n",
    "print(f\"     â†’ Average prediction error magnitude\")\n",
    "print(f\"   MAE (Mean Absolute Error): {mae:.2f} MW\")\n",
    "print(f\"     â†’ Average absolute deviation from actual\")\n",
    "print(f\"   MAPE (Mean Absolute Percentage Error): {mape:.2f}%\")\n",
    "print(f\"     â†’ Percentage error (scale-independent)\")\n",
    "\n",
    "# Sequence Accuracy: How many predictions are within threshold?\n",
    "thresholds = [100, 200, 300]  # MW tolerances\n",
    "print(f\"\\nðŸ“Š SEQUENCE ACCURACY (Critical for Grid Operations)\")\n",
    "print(f\"   'Correct' predictions = within threshold of actual\")\n",
    "for threshold in thresholds:\n",
    "    accuracy = 100 * np.sum(forecast_df['abs_error'] <= threshold) / len(forecast_df)\n",
    "    print(f\"   - Within Â±{threshold} MW: {accuracy:.1f}% of predictions\")\n",
    "\n",
    "# Direction Accuracy: Do we predict the right trend?\n",
    "direction_errors = 0\n",
    "for i in range(1, len(y_test_original)):\n",
    "    actual_trend = np.sign(y_test_original[i] - y_test_original[i-1])\n",
    "    pred_trend = np.sign(y_pred_original[i] - y_pred_original[i-1])\n",
    "    if actual_trend != pred_trend and actual_trend != 0 and pred_trend != 0:\n",
    "        direction_errors += 1\n",
    "\n",
    "direction_accuracy = 100 * (1 - direction_errors / len(y_test_original))\n",
    "print(f\"   - Direction Accuracy (Trend): {direction_accuracy:.1f}%\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ WHY SEQUENCE ACCURACY MATTERS FOR GRID OPERATIONS\")\n",
    "print(f\"   - Grid operators need Â±300 MW accuracy to balance supply/demand\")\n",
    "print(f\"   - Wrong direction forecast can cause blackouts\")\n",
    "print(f\"   - LSTM learns trends better than XGBoost for sequences\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Visualization: 48-hour forecast window\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Full test period\n",
    "axes[0].plot(y_test_original, label='Actual', linewidth=2, color='darkblue')\n",
    "axes[0].plot(y_pred_original, label='LSTM Forecast', linewidth=2, color='darkred', alpha=0.7)\n",
    "axes[0].fill_between(range(len(y_test_original)), \n",
    "                      y_test_original - 200, y_test_original + 200,\n",
    "                      alpha=0.2, color='green', label='Â±200 MW Tolerance')\n",
    "axes[0].set_xlabel('Hours Ahead', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Load (MW)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Full Test Period: Actual vs. LSTM Forecast', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Zoomed 48-hour window\n",
    "zoom_window = 48\n",
    "axes[1].plot(y_test_original[:zoom_window], label='Actual', linewidth=2.5, marker='o', markersize=6, color='darkblue')\n",
    "axes[1].plot(y_pred_original[:zoom_window], label='LSTM Forecast', linewidth=2.5, marker='s', markersize=6, color='darkred')\n",
    "axes[1].fill_between(range(zoom_window), \n",
    "                      y_test_original[:zoom_window] - 200, y_test_original[:zoom_window] + 200,\n",
    "                      alpha=0.2, color='green', label='Â±200 MW Tolerance')\n",
    "axes[1].set_xlabel('Hours Ahead', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Load (MW)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Zoomed 48-Hour Window: Detailed Forecast Accuracy', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nForecast Visualization Insights:\")\n",
    "print(f\"  â†’ Top plot: Overall trend tracking over {len(y_test_original)} hours\")\n",
    "print(f\"  â†’ Bottom plot: 48-hour detailed view showing hourly variations\")\n",
    "print(f\"  â†’ Green band: Acceptable error margin (Â±200 MW) for grid operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c5e4f5",
   "metadata": {},
   "source": [
    "## 11. XGBoost vs. LSTM: The AI Architecture Evolution\n",
    "\n",
    "Comparing our LSTM approach to the XGBoost module from the Boosting folder reveals fundamental differences in the ML paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison: XGBoost vs. LSTM for Time-Series Forecasting\n",
    "comparison_data = {\n",
    "    'Aspect': [\n",
    "        'Interpretability',\n",
    "        'Feature Engineering',\n",
    "        'Long-Range Dependencies',\n",
    "        'Missing Data Handling',\n",
    "        'Training Speed',\n",
    "        'Inference Speed',\n",
    "        'Temporal Patterns',\n",
    "        'Scalability',\n",
    "        'Production Ease',\n",
    "        'Computational Requirements'\n",
    "    ],\n",
    "    'XGBoost (Classical)': [\n",
    "        'EXCELLENT - Feature importance clear',\n",
    "        'Manual - Requires domain knowledge',\n",
    "        'MODERATE - Limited to 100+ steps',\n",
    "        'Native support',\n",
    "        'FAST - Minutes',\n",
    "        'VERY FAST - Milliseconds',\n",
    "        'Good for tabular data',\n",
    "        'Good to ~100K samples',\n",
    "        'EASY - Single file export',\n",
    "        'LOW - CPU only'\n",
    "    ],\n",
    "    'LSTM (Deep Learning)': [\n",
    "        'POOR - Black box (use SHAP)',\n",
    "        'Automatic - Learned representations',\n",
    "        'EXCELLENT - 100+ timesteps easily',\n",
    "        'Requires preprocessing',\n",
    "        'SLOW - Hours to days',\n",
    "        'Fast - Single millisecond',\n",
    "        'EXCELLENT for sequences',\n",
    "        'Excellent - Millions of samples',\n",
    "        'MEDIUM - Docker/Kubernetes',\n",
    "        'HIGH - GPU recommended'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"XGBOOST VS. LSTM: PARADIGM COMPARISON\")\n",
    "print(\"=\" * 100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\nðŸŽ¯ WHEN TO USE EACH APPROACH:\")\n",
    "print(\"\\nXGBoost is Superior When:\")\n",
    "print(\"  âœ“ Features can be engineered manually (domain expertise available)\")\n",
    "print(\"  âœ“ Need interpretable predictions (regulatory requirements)\")\n",
    "print(\"  âœ“ Limited computational resources (edge devices)\")\n",
    "print(\"  âœ“ Dataset < 100K samples\")\n",
    "print(\"  âœ“ Training time is critical\")\n",
    "\n",
    "print(\"\\nLSTM is Superior When:\")\n",
    "print(\"  âœ“ Raw sequential data without manual features\")\n",
    "print(\"  âœ“ Long-range dependencies matter (24+ hour forecast windows)\")\n",
    "print(\"  âœ“ Temporal patterns are complex (multiple seasonal cycles)\")\n",
    "print(\"  âœ“ Large datasets available (>100K samples)\")\n",
    "print(\"  âœ“ GPU/TPU resources available for training\")\n",
    "\n",
    "print(\"\\nðŸ”„ HYBRID APPROACH (Ambient Systems Future):\")\n",
    "print(\"  1. Use LSTM for long-range trend prediction (next 48 hours)\")\n",
    "print(\"  2. Use XGBoost for short-term corrections (anomaly detection)\")\n",
    "print(\"  3. Ensemble both: weight LSTM + XGBoost predictions\")\n",
    "print(\"  4. Fallback to XGBoost if LSTM inference fails\")\n",
    "\n",
    "print(\"\\nðŸ’¡ FOR SMART GRID FORECASTING:\")\n",
    "print(\"  Challenge: Predict load 24-48 hours ahead with Â±200 MW accuracy\")\n",
    "print(\"  â†’ LSTM handles 24+ hour patterns (daily/weekly cycles)\")\n",
    "print(\"  â†’ XGBoost failed at 100+ step horizons\")\n",
    "print(\"  â†’ LSTM is the right tool for this specific problem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eecefc",
   "metadata": {},
   "source": [
    "## 12. Model Deployment: SavedModel Format\n",
    "\n",
    "Save the trained LSTM in TensorFlow's SavedModel format for production deployment via TensorFlow Serving or containerization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5440dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Save model in SavedModel format\n",
    "model_save_path = 'lstm_energy_forecast_model'\n",
    "model.save(model_save_path)\n",
    "\n",
    "print(\"Model Deployment: SavedModel Format\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"âœ“ Model saved to: {model_save_path}/\")\n",
    "print(f\"  Directory structure:\")\n",
    "print(f\"    â”œâ”€â”€ assets/ (e.g., vocabulary files)\")\n",
    "print(f\"    â”œâ”€â”€ saved_model.pb (model graph)\")\n",
    "print(f\"    â”œâ”€â”€ keras_metadata.pb (Keras metadata)\")\n",
    "print(f\"    â””â”€â”€ variables/ (trained weights)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 2: Save scaler for preprocessing\n",
    "import joblib\n",
    "scaler_save_path = 'load_scaler.pkl'\n",
    "joblib.dump(scaler, scaler_save_path)\n",
    "print(f\"\\nâœ“ Scaler saved to: {scaler_save_path}\")\n",
    "\n",
    "# Step 3: Save deployment metadata\n",
    "import json\n",
    "deployment_metadata = {\n",
    "    'model_name': 'LSTM Energy Demand Forecaster',\n",
    "    'model_type': 'Recurrent Neural Network (LSTM)',\n",
    "    'architecture': {\n",
    "        'window_size': int(window_size),\n",
    "        'lstm_layer_1': 64,\n",
    "        'lstm_layer_2': 32,\n",
    "        'dropout_rate': 0.2,\n",
    "        'output_units': 1\n",
    "    },\n",
    "    'training_config': {\n",
    "        'optimizer': 'Adam',\n",
    "        'learning_rate': 0.001,\n",
    "        'loss_function': 'MSE',\n",
    "        'epochs_trained': len(history.history['loss']),\n",
    "        'final_rmse': float(rmse),\n",
    "        'final_mae': float(mae),\n",
    "        'final_mape': float(mape)\n",
    "    },\n",
    "    'input_spec': {\n",
    "        'shape': [None, window_size, 1],\n",
    "        'dtype': 'float32',\n",
    "        'description': '(batch_size, 48 hours, 1 feature)'\n",
    "    },\n",
    "    'output_spec': {\n",
    "        'shape': [None, 1],\n",
    "        'dtype': 'float32',\n",
    "        'range': [500, 3500],\n",
    "        'unit': 'MW'\n",
    "    },\n",
    "    'preprocessing': {\n",
    "        'scaler_type': 'MinMaxScaler',\n",
    "        'feature_range': [0, 1],\n",
    "        'scaler_path': scaler_save_path\n",
    "    },\n",
    "    'deployment_ready': True,\n",
    "    'recommendations': {\n",
    "        'inference_framework': 'TensorFlow Serving',\n",
    "        'containerization': 'Docker with TensorFlow Serving image',\n",
    "        'batch_size': 32,\n",
    "        'expected_latency_ms': 50,\n",
    "        'gpu_recommended': True\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_save_path = 'model_metadata.json'\n",
    "with open(metadata_save_path, 'w') as f:\n",
    "    json.dump(deployment_metadata, f, indent=2)\n",
    "print(f\"âœ“ Metadata saved to: {metadata_save_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DEPLOYMENT INFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "print(json.dumps(deployment_metadata, indent=2))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 4: Demonstrate loading and inference\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PRODUCTION INFERENCE EXAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load saved model\n",
    "loaded_model = keras.models.load_model(model_save_path)\n",
    "print(f\"\\nâœ“ Model loaded from disk\")\n",
    "\n",
    "# Load saved scaler\n",
    "loaded_scaler = joblib.load(scaler_save_path)\n",
    "print(f\"âœ“ Scaler loaded from disk\")\n",
    "\n",
    "# Example: Predict next hour given 48-hour history\n",
    "example_window = X_test[0:1]  # Take first test sample (48 hours)\n",
    "example_pred_scaled = loaded_model.predict(example_window, verbose=0)\n",
    "example_pred_original = loaded_scaler.inverse_transform(example_pred_scaled)[0, 0]\n",
    "\n",
    "print(f\"\\nExample Inference:\")\n",
    "print(f\"  Input: 48 hours of historical load data\")\n",
    "print(f\"  Predicted Load (Hour 49): {example_pred_original:.1f} MW\")\n",
    "print(f\"  Actual Load (Hour 49): {y_test_original[0]:.1f} MW\")\n",
    "print(f\"  Error: {abs(example_pred_original - y_test_original[0]):.1f} MW\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DEPLOYMENT WORKFLOW FOR PRODUCTION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "## Option 1: TensorFlow Serving (Recommended)\n",
    "1. Copy 'lstm_energy_forecast_model/' to serving directory\n",
    "2. Start TensorFlow Serving container:\n",
    "   docker run -p 8500:8500 -p 8501:8501 \\\\\n",
    "     -v /path/to/models:/models \\\\\n",
    "     tensorflow/serving:latest\n",
    "\n",
    "3. Send HTTP/gRPC requests for predictions\n",
    "\n",
    "## Option 2: Docker Containerization\n",
    "1. Create Dockerfile with custom inference script\n",
    "2. Load model and scaler at startup\n",
    "3. Expose REST API endpoint for real-time predictions\n",
    "4. Deploy to Kubernetes cluster\n",
    "\n",
    "## Option 3: Edge Deployment (TensorFlow Lite)\n",
    "1. Convert model: tf.lite.TFLiteConverter\n",
    "2. Deploy on edge devices (Raspberry Pi, industrial IoT)\n",
    "3. Local inference without network latency\n",
    "4. Perfect for distributed smart grid nodes\n",
    "\n",
    "## Input/Output Specification:\n",
    "- Input: 48-hour historical load sequence (scaled to [0,1])\n",
    "- Output: Predicted load for next hour (MW)\n",
    "- Latency: ~50ms (GPU), ~200ms (CPU)\n",
    "- Throughput: 1000+ predictions/second (batched)\n",
    "\"\"\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c21cf3e",
   "metadata": {},
   "source": [
    "## Summary: LSTM for Deep Temporal Forecasting\n",
    "\n",
    "### Journey Through ML Paradigms\n",
    "\n",
    "We've now progressed through the ML evolutionary chain:\n",
    "\n",
    "1. **Classical ML (Random Forest)** - Building Energy Management\n",
    "   - Parallel ensemble trees, good interpretability, ~100-step horizon\n",
    "\n",
    "2. **Boosting (XGBoost)** - Smart Grid Load Forecasting  \n",
    "   - Sequential error correction, better than bagging, still ~100-step limit\n",
    "\n",
    "3. **Deep Learning (LSTM)** - Neural Energy Demand Forecasting\n",
    "   - Recurrent networks with gate mechanisms, learns 100+ timestep dependencies\n",
    "   - Excels at capturing seasonal patterns and trends\n",
    "\n",
    "### Key Technical Insights\n",
    "\n",
    "**The Vanishing Gradient Problem:**\n",
    "- Standard RNNs fail at long sequences due to exponential gradient shrinkage\n",
    "- LSTMs solve this with additive cell state updates and gating\n",
    "- Enables learning of patterns 50-100+ timesteps away\n",
    "\n",
    "**Why LSTM Matters for Grids:**\n",
    "- Daily (24h), weekly (168h), yearly patterns now learnable\n",
    "- Captures extreme weather event responses\n",
    "- Sequence accuracy critical: Â±200 MW tolerance for blackout prevention\n",
    "- 48-hour forecasts enable proactive grid balancing\n",
    "\n",
    "**Trade-offs Accepted:**\n",
    "- âœ— Loss of interpretability (black box predictions)\n",
    "- âœ— Slower training (hours vs. minutes for XGBoost)\n",
    "- âœ— Higher computational cost (GPU needed)\n",
    "- âœ“ Superior raw predictive power on sequences\n",
    "- âœ“ Automatic feature learning\n",
    "- âœ“ Handles missing values naturally\n",
    "\n",
    "### Production Readiness\n",
    "\n",
    "âœ“ Model saved in SavedModel format (TensorFlow Serving compatible)\n",
    "âœ“ Scaler exported for consistent preprocessing\n",
    "âœ“ Metadata documented for deployment teams\n",
    "âœ“ Multiple deployment options (TensorFlow Serving, Docker, Edge)\n",
    "\n",
    "### Ambient Systems Next Steps\n",
    "\n",
    "1. **Deploy to Production**: TensorFlow Serving on Kubernetes\n",
    "2. **Monitor Performance**: Real-time prediction accuracy tracking\n",
    "3. **Ensemble Approaches**: Combine LSTM trend + XGBoost anomalies\n",
    "4. **Federated Learning**: Train on distributed grid nodes\n",
    "5. **Adaptive Retraining**: Seasonal model updates (summer vs. winter)\n",
    "6. **Integration with Control**: Feed forecasts to HVAC optimization algorithms"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
