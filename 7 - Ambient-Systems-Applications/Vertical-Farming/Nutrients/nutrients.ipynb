{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe2069c",
   "metadata": {},
   "source": [
    "# ðŸ’§ RL-Driven Nutrient Feedback for Optimized Hydroponics\n",
    "\n",
    "This notebook provides a \"Golden Path\" for using Reinforcement Learning to adaptively control nutrient dosing (EC/pH) in a hydroponic vertical farm.\n",
    "\n",
    "Why RL? Traditional PID or rule-based controllers are static and can struggle with non-linear disturbances (evaporation, COâ‚‚ spikes, crop stage changes). An RL agent learns by trial-and-error to balance plant health and resource efficiency, reducing chemical overuse and improving yield stability â€” directly translating into sustainability and cost savings.\n",
    "\n",
    "**Scenario:** Adaptive Nutrient Delivery System â€” control discrete dosing actions (increase/hold/decrease) to keep pH and EC in optimal ranges while minimizing resource use and adapting to disturbances.\n",
    "\n",
    "**Contents:** Data simulation (10k steps), environment definition (Gymnasium), baseline PID controller, RL agent training with Stable Baselines3 (PPO), evaluation (rewards, stability, efficiency), policy interpretation, and deployment-ready export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment and run if needed)\n",
    "# !pip install -q stable-baselines3[extra] gymnasium numpy pandas matplotlib seaborn joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5267abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and reproducible settings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# Stable Baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "import math\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a548eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Environment: Hydroponic nutrient control\n",
    "# ------------------------------\n",
    "\n",
    "class HydroponicEnv(gym.Env):\n",
    "    \"\"\"Custom Gymnasium environment simulating nutrient (EC/pH) dynamics in hydroponics.\n",
    "\n",
    "    State: [pH, EC, Temp, Humidity, Plant_Age]\n",
    "    Action: Discrete(3) -> 0: decrease dose, 1: hold, 2: increase dose\n",
    "    Reward: Encourages pH and EC within target ranges and penalizes excessive dosing\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, episode_length=336, seed: int = None):\n",
    "        super().__init__()\n",
    "        self.episode_length = episode_length  # steps per episode (e.g., hours)\n",
    "        self.seed(seed)\n",
    "\n",
    "        # Action space: decrease / hold / increase\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # Observation space: pH [4.5,7.5], EC [0.5,3.5], Temp [12, 32], Humidity [20, 95], Plant_Age [0, 90]\n",
    "        low = np.array([4.5, 0.5, 12.0, 20.0, 0.0], dtype=np.float32)\n",
    "        high = np.array([7.5, 3.5, 32.0, 95.0, 90.0], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float32)\n",
    "\n",
    "        # Optimal ranges\n",
    "        self.pH_opt = (5.8, 6.2)\n",
    "        self.EC_opt = (1.6, 2.0)\n",
    "\n",
    "        # Internal state\n",
    "        self.state = None\n",
    "        self.step_count = 0\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self, *, seed: int = None, options: dict = None):\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "\n",
    "        # Initialize near-optimal with small random offsets\n",
    "        pH = float(6.0 + self.np_random.normal(0, 0.05))\n",
    "        EC = float(1.8 + self.np_random.normal(0, 0.05))\n",
    "        Temp = float(22.0 + self.np_random.normal(0, 1.0))\n",
    "        Humidity = float(60.0 + self.np_random.normal(0, 3.0))\n",
    "        Plant_Age = float(self.np_random.integers(0, 56))  # cycle up to 8 weeks\n",
    "\n",
    "        self.state = np.array([pH, EC, Temp, Humidity, Plant_Age], dtype=np.float32)\n",
    "        self.step_count = 0\n",
    "        return self.state.copy(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        pH, EC, Temp, Humidity, Plant_Age = self.state\n",
    "\n",
    "        # Map discrete action to dosing change\n",
    "        dose_change = {0: -0.03, 1: 0.0, 2: 0.03}[int(action)]\n",
    "\n",
    "        # Nutrient dosing affects EC directly and can nudge pH slightly\n",
    "        EC = float(np.clip(EC + dose_change + self.np_random.normal(0, 0.005), 0.5, 3.5))\n",
    "        pH = float(np.clip(pH - 0.01 * dose_change + self.np_random.normal(0, 0.02), 4.5, 7.5))\n",
    "\n",
    "        # Natural drift: evaporation increases EC slowly; occasional spikes disrupt pH\n",
    "        EC += 0.0005  # slow accumulation\n",
    "        if self.np_random.rand() < 0.01:  # spike event (CO2 injection / contamination)\n",
    "            pH += self.np_random.normal(0.25, 0.1)\n",
    "\n",
    "        # Temperature and humidity drift slowly\n",
    "        Temp = float(np.clip(Temp + self.np_random.normal(0, 0.05), 12, 32))\n",
    "        Humidity = float(np.clip(Humidity + self.np_random.normal(0, 0.2), 20, 95))\n",
    "\n",
    "        # Plant age increments\n",
    "        Plant_Age = float(Plant_Age + 1 / 24)  # hourly increment\n",
    "\n",
    "        self.state = np.array([pH, EC, Temp, Humidity, Plant_Age], dtype=np.float32)\n",
    "        self.step_count += 1\n",
    "\n",
    "        # Reward: keep pH and EC within optimal ranges, penalize large dosing\n",
    "        pH_in = float(self.pH_opt[0] <= pH <= self.pH_opt[1])\n",
    "        EC_in = float(self.EC_opt[0] <= EC <= self.EC_opt[1])\n",
    "\n",
    "        # Stability reward component (closeness to center)\n",
    "        pH_center = 1.0 - abs((pH - np.mean(self.pH_opt)) / 1.0)\n",
    "        EC_center = 1.0 - abs((EC - np.mean(self.EC_opt)) / 1.0)\n",
    "\n",
    "        # Action cost (resource efficiency)\n",
    "        action_cost = 0.02 if action != 1 else 0.0\n",
    "\n",
    "        reward = 0.5 * (pH_in + EC_in) + 0.3 * (pH_center + EC_center) - action_cost\n",
    "\n",
    "        # Penalty for large deviations\n",
    "        if pH < 5.0 or pH > 7.0:\n",
    "            reward -= 0.5\n",
    "        if EC < 0.8 or EC > 3.0:\n",
    "            reward -= 0.5\n",
    "\n",
    "        done = self.step_count >= self.episode_length\n",
    "\n",
    "        info = {\n",
    "            'pH': pH,\n",
    "            'EC': EC,\n",
    "            'Temp': Temp,\n",
    "            'Humidity': Humidity,\n",
    "            'Plant_Age': Plant_Age\n",
    "        }\n",
    "\n",
    "        return self.state.copy(), float(reward), bool(done), False, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Step {self.step_count} | pH={self.state[0]:.2f} | EC={self.state[1]:.2f}\")\n",
    "\n",
    "\n",
    "# Quick sanity check\n",
    "env = HydroponicEnv(episode_length=168, seed=SEED)\n",
    "obs, _ = env.reset(seed=SEED)\n",
    "print('Initial observation:', obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c331cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Data simulation: generate 10,000-step dataset using a simple PID-like controller\n",
    "# ------------------------------\n",
    "\n",
    "def pid_controller(state, pH_target=6.0, EC_target=1.8):\n",
    "    \"\"\"Simple proportional controller mapping error to discrete action.\"\"\"\n",
    "    pH, EC, Temp, Humidity, Plant_Age = state\n",
    "    # Combine errors (pH and EC) to decide an action\n",
    "    pH_err = pH_target - pH\n",
    "    EC_err = EC_target - EC\n",
    "\n",
    "    # Weighted error\n",
    "    err = 0.6 * pH_err + 0.4 * (EC_err / 1.0)\n",
    "\n",
    "    # Proportional mapping to discrete action\n",
    "    if err > 0.05:\n",
    "        return 2  # increase\n",
    "    elif err < -0.05:\n",
    "        return 0  # decrease\n",
    "    else:\n",
    "        return 1  # hold\n",
    "\n",
    "\n",
    "def generate_dataset(env, controller_fn, steps=10000, seed=SEED):\n",
    "    data = []\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    for t in range(steps):\n",
    "        action = controller_fn(obs)\n",
    "        next_obs, reward, done, _, info = env.step(action)\n",
    "        data.append({\n",
    "            'timestamp': t,\n",
    "            'pH': obs[0],\n",
    "            'EC': obs[1],\n",
    "            'Temp': obs[2],\n",
    "            'Humidity': obs[3],\n",
    "            'Plant_Age': obs[4],\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "        })\n",
    "        obs = next_obs\n",
    "        if done:\n",
    "            obs, _ = env.reset()\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Use a fresh env for dataset generation to avoid interfering with training env state\n",
    "sim_env = HydroponicEnv(episode_length=1000, seed=SEED)\n",
    "df_sim = generate_dataset(sim_env, pid_controller, steps=10000)\n",
    "\n",
    "print('Dataset shape:', df_sim.shape)\n",
    "df_sim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce9d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a segment of the simulated data\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(df_sim['timestamp'].iloc[:1000], df_sim['pH'].iloc[:1000], label='pH')\n",
    "plt.plot(df_sim['timestamp'].iloc[:1000], df_sim['EC'].iloc[:1000], label='EC')\n",
    "plt.axhspan(5.8, 6.2, color='green', alpha=0.1, label='pH optimal')\n",
    "plt.axhspan(1.6, 2.0, color='blue', alpha=0.05, label='EC optimal')\n",
    "plt.legend()\n",
    "plt.title('Simulated nutrient time-series (first 1000 steps)')\n",
    "plt.xlabel('Step')\n",
    "plt.show()\n",
    "\n",
    "# Quick stats\n",
    "df_sim[['pH', 'EC', 'action', 'reward']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d194d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Baseline: PID controller evaluation over multiple episodes\n",
    "# ------------------------------\n",
    "\n",
    "def evaluate_controller(env, controller_fn, episodes=30):\n",
    "    metrics = []\n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0.0\n",
    "        ph_vals = []\n",
    "        ec_vals = []\n",
    "        actions = []\n",
    "        for t in range(env.episode_length):\n",
    "            action = controller_fn(obs)\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            ph_vals.append(info['pH'])\n",
    "            ec_vals.append(info['EC'])\n",
    "            actions.append(action)\n",
    "            if done:\n",
    "                break\n",
    "        metrics.append({\n",
    "            'episode': ep,\n",
    "            'total_reward': total_reward,\n",
    "            'pH_var': np.var(ph_vals),\n",
    "            'EC_var': np.var(ec_vals),\n",
    "            'mean_actions': np.mean(actions)\n",
    "        })\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "baseline_env = HydroponicEnv(episode_length=168, seed=SEED)\n",
    "baseline_metrics = evaluate_controller(baseline_env, pid_controller, episodes=40)\n",
    "print('Baseline mean reward:', baseline_metrics['total_reward'].mean())\n",
    "baseline_metrics.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a41f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Train RL agent (PPO) with Monitor and evaluation callback\n",
    "# ------------------------------\n",
    "\n",
    "train_env = Monitor(HydroponicEnv(episode_length=168, seed=SEED))\n",
    "vec_env = DummyVecEnv([lambda: train_env])\n",
    "\n",
    "policy_kwargs = dict(net_arch=[dict(pi=[64, 64], vf=[64, 64])])\n",
    "model = PPO('MlpPolicy', vec_env, verbose=1, seed=SEED, policy_kwargs=policy_kwargs)\n",
    "\n",
    "# Evaluate every 5k timesteps on a separate eval env\n",
    "eval_env = Monitor(HydroponicEnv(episode_length=168, seed=SEED + 1))\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/', log_path='./logs/', eval_freq=5000, n_eval_episodes=5, deterministic=True)\n",
    "\n",
    "# Train (short demo training; increase timesteps for production)\n",
    "TIMESTEPS = 50000\n",
    "model.learn(total_timesteps=TIMESTEPS, callback=eval_callback)\n",
    "\n",
    "# Save final policy\n",
    "model.save('nutrient_agent')\n",
    "print('Training complete; model saved as nutrient_agent.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b7755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Evaluate trained agent vs baseline PID: reward curves and stability\n",
    "# ------------------------------\n",
    "\n",
    "def run_episode_collect(env, agent=None, controller_fn=None):\n",
    "    obs, _ = env.reset()\n",
    "    rewards = []\n",
    "    ph_vals = []\n",
    "    ec_vals = []\n",
    "    actions = []\n",
    "    for t in range(env.episode_length):\n",
    "        if agent is not None:\n",
    "            action, _ = agent.predict(obs, deterministic=True)\n",
    "            action = int(action)\n",
    "        else:\n",
    "            action = controller_fn(obs)\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        ph_vals.append(info['pH'])\n",
    "        ec_vals.append(info['EC'])\n",
    "        actions.append(action)\n",
    "        if done:\n",
    "            break\n",
    "    return {\n",
    "        'rewards': np.array(rewards),\n",
    "        'pH': np.array(ph_vals),\n",
    "        'EC': np.array(ec_vals),\n",
    "        'actions': np.array(actions)\n",
    "    }\n",
    "\n",
    "# Evaluate on several episodes\n",
    "eval_env = HydroponicEnv(episode_length=336, seed=SEED + 2)\n",
    "agent_results = run_episode_collect(eval_env, agent=model)\n",
    "\n",
    "# Reset eval env and run PID baseline on same env type\n",
    "eval_env2 = HydroponicEnv(episode_length=336, seed=SEED + 3)\n",
    "pid_results = run_episode_collect(eval_env2, controller_fn=pid_controller)\n",
    "\n",
    "# Plot episode rewards\n",
    "plt.plot(np.cumsum(agent_results['rewards']), label='PPO (agent)')\n",
    "plt.plot(np.cumsum(pid_results['rewards']), label='PID baseline')\n",
    "plt.title('Cumulative Episode Reward (single episode comparison)')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Stability metrics\n",
    "def stability_metrics(results):\n",
    "    return {\n",
    "        'avg_reward': results['rewards'].sum(),\n",
    "        'pH_var': float(np.var(results['pH'])),\n",
    "        'EC_var': float(np.var(results['EC'])),\n",
    "        'resource_use': float(np.sum(results['actions'] != 1))  # count non-hold actions\n",
    "    }\n",
    "\n",
    "agent_metrics = stability_metrics(agent_results)\n",
    "pid_metrics = stability_metrics(pid_results)\n",
    "\n",
    "pd.DataFrame([agent_metrics, pid_metrics], index=['PPO_agent', 'PID_baseline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# State-Action heatmap and policy interpretation\n",
    "# ------------------------------\n",
    "\n",
    "# Collect a longer set of (pH, action) pairs from the trained agent\n",
    "def collect_state_action(agent, env, steps=2000):\n",
    "    obs, _ = env.reset()\n",
    "    phs = []\n",
    "    ecs = []\n",
    "    acts = []\n",
    "    for _ in range(steps):\n",
    "        action, _ = agent.predict(obs, deterministic=True)\n",
    "        action = int(action)\n",
    "        obs, _, done, _, info = env.step(action)\n",
    "        phs.append(info['pH'])\n",
    "        ecs.append(info['EC'])\n",
    "        acts.append(action)\n",
    "        if done:\n",
    "            obs, _ = env.reset()\n",
    "    return np.array(phs), np.array(ecs), np.array(acts)\n",
    "\n",
    "phs, ecs, acts = collect_state_action(model, HydroponicEnv(episode_length=1000, seed=SEED + 4), steps=3000)\n",
    "\n",
    "# Heatmap by pH bins\n",
    "bins = np.linspace(4.5, 7.5, 31)\n",
    "ph_bin_idx = np.digitize(phs, bins) - 1\n",
    "heat = np.zeros((len(bins), 3))\n",
    "for b, a in zip(ph_bin_idx, acts):\n",
    "    if 0 <= b < len(bins):\n",
    "        heat[b, a] += 1\n",
    "\n",
    "# Normalize per bin\n",
    "heat_norm = (heat.T / (heat.sum(axis=1) + 1e-8)).T\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.imshow(heat_norm.T, aspect='auto', origin='lower', extent=[bins[0], bins[-1], -0.5, 2.5], cmap='viridis')\n",
    "plt.yticks([0, 1, 2], ['dec', 'hold', 'inc'])\n",
    "plt.colorbar(label='Fraction of actions')\n",
    "plt.xlabel('pH')\n",
    "plt.title('State-Action heatmap (agent): action fraction by pH')\n",
    "plt.show()\n",
    "\n",
    "# Simple operator rule extraction (conditional averages)\n",
    "rule_df = pd.DataFrame({'pH': phs, 'EC': ecs, 'action': acts})\n",
    "# Compute mean action at coarse pH buckets\n",
    "rule_summary = rule_df.groupby(pd.cut(rule_df['pH'], bins=np.linspace(4.5, 7.5, 7)))['action'].mean()\n",
    "print('Rule summary (mean discrete action by pH bucket):')\n",
    "print(rule_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb76ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Simulate agent vs PID on same initial seed and overlay pH/EC profiles\n",
    "# ------------------------------\n",
    "\n",
    "seeded_env_agent = HydroponicEnv(episode_length=336, seed=1234)\n",
    "seeded_env_pid = HydroponicEnv(episode_length=336, seed=1234)\n",
    "\n",
    "agent_run = run_episode_collect(seeded_env_agent, agent=model)\n",
    "pid_run = run_episode_collect(seeded_env_pid, controller_fn=pid_controller)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(agent_run['pH'], label='PPO pH')\n",
    "plt.plot(pid_run['pH'], label='PID pH', alpha=0.8)\n",
    "plt.axhspan(5.8, 6.2, color='green', alpha=0.1)\n",
    "plt.legend()\n",
    "plt.title('pH â€” Agent vs PID (same seed)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(agent_run['EC'], label='PPO EC')\n",
    "plt.plot(pid_run['EC'], label='PID EC', alpha=0.8)\n",
    "plt.axhspan(1.6, 2.0, color='blue', alpha=0.05)\n",
    "plt.legend()\n",
    "plt.title('EC â€” Agent vs PID (same seed)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee85cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Business-friendly evaluation metrics and interpretations\n",
    "# ------------------------------\n",
    "\n",
    "# Average Reward per Episode (higher is better)\n",
    "agent_eval = []\n",
    "for i in range(10):\n",
    "    res = run_episode_collect(HydroponicEnv(episode_length=336, seed=SEED + 10 + i), agent=model)\n",
    "    agent_eval.append(res['rewards'].sum())\n",
    "\n",
    "baseline_eval = []\n",
    "for i in range(10):\n",
    "    res = run_episode_collect(HydroponicEnv(episode_length=336, seed=SEED + 100 + i), controller_fn=pid_controller)\n",
    "    baseline_eval.append(res['rewards'].sum())\n",
    "\n",
    "print('Avg reward (agent):', np.mean(agent_eval))\n",
    "print('Avg reward (PID):', np.mean(baseline_eval))\n",
    "\n",
    "# Stability index: lower variance in pH and EC is better\n",
    "agent_stability = []\n",
    "baseline_stability = []\n",
    "for i in range(10):\n",
    "    a = run_episode_collect(HydroponicEnv(episode_length=336, seed=SEED + 20 + i), agent=model)\n",
    "    p = run_episode_collect(HydroponicEnv(episode_length=336, seed=SEED + 120 + i), controller_fn=pid_controller)\n",
    "    agent_stability.append(np.var(a['pH']) + np.var(a['EC']))\n",
    "    baseline_stability.append(np.var(p['pH']) + np.var(p['EC']))\n",
    "\n",
    "print('Stability index (agent mean):', np.mean(agent_stability))\n",
    "print('Stability index (PID mean):', np.mean(baseline_stability))\n",
    "\n",
    "# Efficiency score: resource use per unit reward (lower is better)\n",
    "agent_eff = []\n",
    "baseline_eff = []\n",
    "for i in range(10):\n",
    "    a = run_episode_collect(HydroponicEnv(episode_length=336, seed=SEED + 30 + i), agent=model)\n",
    "    p = run_episode_collect(HydroponicEnv(episode_length=336, seed=SEED + 130 + i), controller_fn=pid_controller)\n",
    "    agent_eff.append(np.sum(a['actions'] != 1) / (np.sum(a['rewards']) + 1e-6))\n",
    "    baseline_eff.append(np.sum(p['actions'] != 1) / (np.sum(p['rewards']) + 1e-6))\n",
    "\n",
    "print('Resource per reward (agent mean):', np.mean(agent_eff))\n",
    "print('Resource per reward (PID mean):', np.mean(baseline_eff))\n",
    "\n",
    "print('\\nBusiness interpretation:')\n",
    "print('- Higher average reward indicates better overall nutrient health control (less crop stress).')\n",
    "print('- Lower stability index means pH/EC remain steadier, reducing crop failure risk and manual interventions.')\n",
    "print('- Efficiency score balances dosing frequency against control quality; lower is more economical.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a10806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Export & Deployment Ready\n",
    "# ------------------------------\n",
    "\n",
    "# Save agent\n",
    "model.save('nutrient_agent')\n",
    "print('Agent saved: nutrient_agent.zip')\n",
    "\n",
    "# Sample inference snippet for edge device / real-time\n",
    "\n",
    "sample_obs, _ = HydroponicEnv().reset()\n",
    "print('Sample obs:', sample_obs)\n",
    "\n",
    "# In production you would load and call:\n",
    "# from stable_baselines3 import PPO\n",
    "# agent = PPO.load('nutrient_agent')\n",
    "# action, _ = agent.predict(observation, deterministic=True)\n",
    "# then send action to actuator (pump control) with safeguards/limits\n",
    "\n",
    "# Deployment checklist (brief):\n",
    "# - Wrap the model in a lightweight service (FastAPI) that receives sensor JSON and returns action + confidence\n",
    "# - Add safety checks & action rate limits to avoid hardware stress (e.g., max 1 dose change / hour)\n",
    "# - Implement logging, anomaly detection, and model drift monitoring\n",
    "\n",
    "print('\\nDeployment ready: model saved and sample inference snippet included.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
