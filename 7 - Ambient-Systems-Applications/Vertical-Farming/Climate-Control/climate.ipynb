{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b8a8f8",
   "metadata": {},
   "source": [
    "# ❄️ LSTM for Predictive Climate Control in Vertical Farms\n",
    "\n",
    "This notebook presents a \"Golden Path\" for forecasting climate variables (Temperature, Humidity, CO₂) using stacked LSTM models. Accurate short-term forecasts (next-hour) enable proactive HVAC/ventilation adjustments to maintain ideal growth conditions and to save energy.\n",
    "\n",
    "Why LSTM? Classic RNNs suffer from vanishing gradients and struggle with long-term dependencies (diurnal/seasonal cycles). LSTM gates (input/forget/output) preserve and update long-range information, making them well-suited for farm climate time-series forecasting.\n",
    "\n",
    "Scenario: \"Intelligent Climate Regulation\" — simulate 10,000 hourly steps of climate data, train LSTM to predict the next-hour climate state, compare to baselines (persistence and a simple RNN), show business-impact metrics (Energy Savings Score), and provide deployment guidance (TF Lite conversion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2fb5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment to run if needed)\n",
    "# !pip install -q tensorflow scikit-learn pandas matplotlib seaborn joblib shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007f5aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports and reproducible settings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, SimpleRNN\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Seeding\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681b8f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Data Simulation: 10,000 hourly steps (~1 year)\n",
    "# ------------------------------\n",
    "\n",
    "def simulate_climate_data(steps=10000, start_date='2024-01-01'):\n",
    "    rng = pd.date_range(start=start_date, periods=steps, freq='H')\n",
    "    df = pd.DataFrame({'timestamp': rng})\n",
    "\n",
    "    hours = np.arange(steps)\n",
    "    # Daily cycle for interior temp driven by lights + diurnal\n",
    "    daily = 3 * np.sin(2 * np.pi * (hours % 24) / 24 - 0.5)\n",
    "    weekly = 0.5 * np.sin(2 * np.pi * hours / (24 * 7))\n",
    "    seasonal = 2.0 * np.sin(2 * np.pi * hours / (24 * 365.0))\n",
    "\n",
    "    # External temperature (weather)\n",
    "    df['External_Temp'] = 5 + 10 * np.sin(2 * np.pi * hours / (24 * 365.0)) + np.random.normal(0, 1.0, steps)\n",
    "\n",
    "    # Internal temperature influenced by lights, external temp, and HVAC\n",
    "    df['Light_Intensity'] = np.clip(200 + 150 * np.sin(2 * np.pi * (hours % 24) / 24) + np.random.normal(0, 20, steps), 0, 1000)\n",
    "    df['Temp'] = np.clip(21 + daily + 0.1 * (df['External_Temp'] - 10) + seasonal + np.random.normal(0, 0.5, steps), 10, 35)\n",
    "\n",
    "    # Humidity inversely related to temperature with some randomness\n",
    "    df['Humidity'] = np.clip(60 - 0.6 * daily + 3 * weekly + np.random.normal(0, 2.0, steps), 20, 95)\n",
    "\n",
    "    # CO2 with enrichment cycles and weekday patterns (ventilation schedule)\n",
    "    df['CO2'] = np.clip(400 + 80 * (0.5 + 0.5 * np.sign(np.sin(2 * np.pi * (hours % 24) / 24))) + 10 * weekly + np.random.normal(0, 15, steps), 300, 2000)\n",
    "\n",
    "    # Plant density (affects HVAC load) cycles with planting schedule\n",
    "    df['Plant_Density'] = np.clip(0.8 + 0.2 * np.sin(2 * np.pi * hours / (24 * 28)) + np.random.normal(0, 0.02, steps), 0.5, 1.2)\n",
    "\n",
    "    # Add noise and occasional disturbance (e.g., door open causing spike)\n",
    "    disturbances = np.random.choice(steps, size=int(steps * 0.002), replace=False)\n",
    "    df.loc[disturbances, 'Temp'] += np.random.uniform(2, 6, size=len(disturbances))\n",
    "    df.loc[disturbances, 'Humidity'] += np.random.uniform(-8, 8, size=len(disturbances))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Generate dataset\n",
    "df = simulate_climate_data(steps=10000)\n",
    "print('Dataset generated:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fb6df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick EDA: visualize temperature and humidity over time (daily avg)\n",
    "df_plot = df.set_index('timestamp').resample('D').mean()\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(df_plot.index, df_plot['Temp'], label='Temp (daily avg)')\n",
    "plt.plot(df_plot.index, df_plot['Humidity'], label='Humidity (daily avg)')\n",
    "plt.title('Daily-Average Temp & Humidity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Show sample rows\n",
    "df[['timestamp', 'Temp', 'Humidity', 'CO2', 'External_Temp', 'Light_Intensity']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdf32ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Preprocessing: sliding window generator, scaling, and train/val/test split\n",
    "# ------------------------------\n",
    "\n",
    "target_cols = ['Temp', 'Humidity', 'CO2']\n",
    "feature_cols = ['Temp', 'Humidity', 'CO2', 'Light_Intensity', 'External_Temp', 'Plant_Density']\n",
    "\n",
    "# Scaling features\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = df.copy()\n",
    "df_scaled[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "\n",
    "# Sliding window\n",
    "def create_sequences(data, feature_cols, target_cols, lookback=24):\n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(data)):\n",
    "        X.append(data[feature_cols].iloc[i - lookback:i].values)\n",
    "        y.append(data[target_cols].iloc[i].values)  # next-step prediction\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "LOOKBACK = 24  # 24 hours\n",
    "X, y = create_sequences(df_scaled, feature_cols, target_cols, lookback=LOOKBACK)\n",
    "print('X shape:', X.shape, 'y shape:', y.shape)\n",
    "\n",
    "# Time-based split\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.15 * len(X))\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]\n",
    "X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]\n",
    "\n",
    "print('Train/Val/Test shapes:', X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6793df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Baselines: Persistence and Simple RNN\n",
    "# ------------------------------\n",
    "\n",
    "# Persistence baseline: predict last observed value (t) as t+1\n",
    "def persistence_predict(X):\n",
    "    # last time-step in sequence\n",
    "    return X[:, -1, :3]  # Temp, Humidity, CO2\n",
    "\n",
    "persistence_preds = persistence_predict(X_test)\n",
    "\n",
    "# Simple RNN baseline\n",
    "rnn_model = Sequential([\n",
    "    SimpleRNN(32, input_shape=(LOOKBACK, X_train.shape[2])),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3)\n",
    "])\n",
    "\n",
    "rnn_model.compile(optimizer='adam', loss='mse')\n",
    "rnn_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=64, verbose=1)\n",
    "\n",
    "rnn_preds = rnn_model.predict(X_test)\n",
    "\n",
    "# Baseline metrics helper\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    metrics = {}\n",
    "    for i, col in enumerate(target_cols):\n",
    "        rmse = np.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i]))\n",
    "        mae = mean_absolute_error(y_true[:, i], y_pred[:, i])\n",
    "        metrics[col] = {'RMSE': rmse, 'MAE': mae}\n",
    "    return metrics\n",
    "\n",
    "persistence_metrics = compute_metrics(y_test, persistence_preds)\n",
    "rnn_metrics = compute_metrics(y_test, rnn_preds)\n",
    "print('Persistence metrics:', persistence_metrics)\n",
    "print('RNN metrics:', rnn_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365f9dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# LSTM model: stacked LSTM with Dropout\n",
    "# ------------------------------\n",
    "\n",
    "def build_lstm(input_shape, units=[64, 32], dropout=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units[0], return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(units[1]))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(len(target_cols)))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "lstm = build_lstm((LOOKBACK, X_train.shape[2]), units=[128, 64], dropout=0.2)\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_lstm.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "early = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "\n",
    "history = lstm.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=80, batch_size=128, callbacks=[checkpoint, early], verbose=2)\n",
    "\n",
    "# Plot training/validation loss\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.title('Training / Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Predict on test\n",
    "lstm_preds = lstm.predict(X_test)\n",
    "lstm_metrics = compute_metrics(y_test, lstm_preds)\n",
    "print('LSTM metrics:', lstm_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b48e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Forecast overlay: visualize a 48-hour zoom on test set\n",
    "# ------------------------------\n",
    "\n",
    "# Pick a test slice near the start of X_test\n",
    "idx = 200\n",
    "hours = np.arange(48)\n",
    "\n",
    "true_slice = y_test[idx:idx+48]\n",
    "pred_slice = lstm_preds[idx:idx+48]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(hours, true_slice[:, 0], label='Actual Temp')\n",
    "plt.plot(hours, pred_slice[:, 0], label='Pred Temp')\n",
    "plt.title('48-hour Forecast — Temp (Actual vs Predicted)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(hours, true_slice[:, 1], label='Actual Humidity')\n",
    "plt.plot(hours, pred_slice[:, 1], label='Pred Humidity')\n",
    "plt.title('Humidity (Actual vs Predicted)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(hours, true_slice[:, 2], label='Actual CO2')\n",
    "plt.plot(hours, pred_slice[:, 2], label='Pred CO2')\n",
    "plt.title('CO2 (Actual vs Predicted)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c9fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Feature importance via permutation (test set)\n",
    "# ------------------------------\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def permutation_importance(model_predict_fn, X_test, y_test, feature_idx, n_repeats=5, metric_fn=None):\n",
    "    if metric_fn is None:\n",
    "        def metric_fn(y_true, y_pred):\n",
    "            return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    base_pred = model_predict_fn(X_test)\n",
    "    base_score = metric_fn(y_test, base_pred)\n",
    "\n",
    "    scores = []\n",
    "    for _ in range(n_repeats):\n",
    "        X_perm = deepcopy(X_test)\n",
    "        # permute a single feature across time (flatten then reshape)\n",
    "        flat = X_perm[:, :, feature_idx].flatten()\n",
    "        np.random.shuffle(flat)\n",
    "        X_perm[:, :, feature_idx] = flat.reshape(X_perm.shape[0], X_perm.shape[1])\n",
    "        perm_pred = model_predict_fn(X_perm)\n",
    "        scores.append(metric_fn(y_test, perm_pred))\n",
    "    return np.mean(scores) - base_score\n",
    "\n",
    "# Model predict wrapper\n",
    "def lstm_predict_fn(X_in):\n",
    "    return lstm.predict(X_in)\n",
    "\n",
    "# Compute permutation importance for each feature (using first frame value as representative)\n",
    "feature_names = feature_cols\n",
    "importances = {}\n",
    "for i, fname in enumerate(feature_names):\n",
    "    # Use column index i in feature dimension\n",
    "    imp = permutation_importance(lstm_predict_fn, X_test, y_test, i, n_repeats=3,\n",
    "                                 metric_fn=lambda y_true, y_pred: np.mean(np.sqrt(np.mean((y_true - y_pred) ** 2, axis=0))))\n",
    "    importances[fname] = imp\n",
    "\n",
    "imp_df = pd.DataFrame({'feature': list(importances.keys()), 'importance': list(importances.values())}).sort_values('importance', ascending=False)\n",
    "print('Permutation importances (higher → worse when permuted):')\n",
    "print(imp_df)\n",
    "\n",
    "# Plot top features\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(data=imp_df.head(8), x='importance', y='feature', palette='magma')\n",
    "plt.title('Permutation Feature Importance (LSTM)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2db2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Business metrics: RMSE/MAE and Energy Savings Score\n",
    "# ------------------------------\n",
    "\n",
    "# Helper to compute aggregated RMSE and MAE\n",
    "def aggregate_metrics(y_true, y_pred):\n",
    "    out = {}\n",
    "    for i, col in enumerate(target_cols):\n",
    "        out[f'{col}_RMSE'] = np.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i]))\n",
    "        out[f'{col}_MAE'] = mean_absolute_error(y_true[:, i], y_pred[:, i])\n",
    "    return out\n",
    "\n",
    "metrics_persistence = aggregate_metrics(y_test, persistence_preds)\n",
    "metrics_rnn = aggregate_metrics(y_test, rnn_preds)\n",
    "metrics_lstm = aggregate_metrics(y_test, lstm_preds)\n",
    "\n",
    "print('Persistence:', metrics_persistence)\n",
    "print('RNN:', metrics_rnn)\n",
    "print('LSTM:', metrics_lstm)\n",
    "\n",
    "# Energy Savings Score: heuristic that better forecasts reduce HVAC energy by proportion of RMSE reduction\n",
    "# Score = 100 * (1 - RMSE_model / RMSE_persistence) averaged over targets (higher is better)\n",
    "rmse_persistence = np.mean([metrics_persistence[f'{c}_RMSE'] for c in target_cols])\n",
    "rmse_lstm = np.mean([metrics_lstm[f'{c}_RMSE'] for c in target_cols])\n",
    "energy_savings_pct = 100 * (1 - rmse_lstm / rmse_persistence)\n",
    "print(f'Estimated Energy Savings (heuristic): {energy_savings_pct:.2f}% (higher is better)')\n",
    "\n",
    "print('\\nBusiness interpretation:')\n",
    "print('- Lower RMSE/MAE reduces over/under-cooling events, translating to HVAC runtime savings.')\n",
    "print('- Energy Savings Score is a heuristic estimate; field calibration required to map RMSE improvements to kWh and cost savings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71147960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Export model and Deployment Blueprint\n",
    "# ------------------------------\n",
    "\n",
    "# Save Keras model\n",
    "lstm.save('climate_lstm.h5')\n",
    "print('Saved model to climate_lstm.h5')\n",
    "\n",
    "# Quick TensorFlow Lite conversion snippet (for edge deployment)\n",
    "try:\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(lstm)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "    with open('climate_lstm.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    print('Saved TFLite model to climate_lstm.tflite')\n",
    "except Exception as e:\n",
    "    print('TFLite conversion skipped (requires full TF environment):', e)\n",
    "\n",
    "# Sample inference on live sensor data (pseudo)\n",
    "# Assume we have last 24 hours as `recent_window` scaled by same scaler\n",
    "# recent_window shape expected: (1, LOOKBACK, n_features)\n",
    "# pred = lstm.predict(recent_window)\n",
    "# Inverse transform if needed and use prediction to trigger HVAC adjustments (pre-cooling, ventilation)\n",
    "\n",
    "# Deployment checklist:\n",
    "# - Export model and quantize for edge (TFLite), wrap in a service (Lightweight or MCU SDK)\n",
    "# - Integrate with building energy models to convert forecast accuracy to runtime adjustments and kWh savings\n",
    "# - Add safety constraints (max cooling rate), fallback (persistence) and human-in-loop overrides\n",
    "\n",
    "print('\\nDeployment blueprint included.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f688983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Actionable Forecasting & Operator Guidance (Ambient differentiation)\n",
    "# ------------------------------\n",
    "\n",
    "# Example heuristic: derive rules from forecasted temp/Humidity\n",
    "print('Actionable rules (examples):')\n",
    "print('- If forecasted Temp in 4h > 25°C: schedule pre-cooling in next 2h to reduce HVAC ramp, estimated energy save ~10-15%')\n",
    "print('- If Humidity trending up and forecasted >80%: increase ventilation and decrease misting to prevent mold risk')\n",
    "print('- If CO2 forecast shows prolonged low levels: postpone enrichment and prioritize recirculation to save CO2 consumption')\n",
    "\n",
    "# What-if simulation cell (operator demo): apply a sudden external temp spike and observe predicted response\n",
    "spike_df = df.copy()\n",
    "# inject a heat spike at t=2000 lasting 12 hours\n",
    "spike_df.loc[2000:2011, 'External_Temp'] += 8\n",
    "# Create sequences for spike window and show model predictions overlay for that region as a demo\n",
    "X_spike, y_spike = create_sequences(spike_df, feature_cols, target_cols, lookback=LOOKBACK)\n",
    "spike_pred = lstm.predict(X_spike[2000:2050])\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_spike[2000:2050, 0], label='Actual Temp (spike region)')\n",
    "plt.plot(spike_pred[:, 0], label='Pred Temp')\n",
    "plt.title('What-if: External Temp Spike — Model Forecast')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
