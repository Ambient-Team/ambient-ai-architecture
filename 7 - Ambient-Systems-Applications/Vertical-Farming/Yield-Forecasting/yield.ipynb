{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a019292",
   "metadata": {},
   "source": [
    "# ðŸŒ± Vertical Farming Yield Forecasting with XGBoost\n",
    "\n",
    "This notebook demonstrates a \"Golden Path\" for predicting leafy-green harvest yield in a multi-layer vertical farm using modern ensemble methods (XGBoost) and explainability (SHAP). Accurate yield forecasts are critical for profitability, harvest planning, labor scheduling, and reducing food waste in controlled environment agriculture (CEA). We'll:\n",
    "\n",
    "- Simulate a realistic 18-month IoT dataset (6,000 rows) with daily/weekly/seasonal signals\n",
    "- Engineer features (rolling windows, time features)\n",
    "- Compare a baseline model (Random Forest) with XGBoost\n",
    "- Perform hyperparameter tuning and evaluate using MAE, RMSE, RÂ², and MAPE\n",
    "- Explain model drivers with SHAP and actionable business insights\n",
    "- Export the trained model for production use\n",
    "\n",
    "**Author:** Senior AI Architect â€” Ambient Systems\n",
    "**Scenario:** Vertical Farm Yield Prediction â€” Predict harvest weight (g) of leafy greens using sensor data (Light, Temp, Humidity, COâ‚‚, EC, pH, Plant Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a6c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment to run if packages are not present)\n",
    "# Use quiet installs to keep notebook outputs clean\n",
    "\n",
    "!pip install -q xgboost shap joblib scikit-learn matplotlib seaborn pandas numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886cc077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports and reproducible settings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "\n",
    "# For SHAP explainability\n",
    "import shap\n",
    "\n",
    "# Plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05effd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Data Simulation â€” realistic 18-month dataset (6,000 rows)\n",
    "# ------------------------------\n",
    "\n",
    "def simulate_vertical_farm_data(n_rows=6000, start_date='2024-07-01', seed=RANDOM_STATE):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Create a time index spanning ~18 months\n",
    "    start = pd.to_datetime(start_date)\n",
    "    end = start + pd.DateOffset(months=18)\n",
    "\n",
    "    # Create timestamps evenly spaced (n_rows points)\n",
    "    timestamps = pd.date_range(start, end, periods=n_rows)\n",
    "    df = pd.DataFrame({'timestamp': timestamps})\n",
    "\n",
    "    # Time features: hour, day, month, dayofweek\n",
    "    df['hour'] = df['timestamp'].dt.hour + df['timestamp'].dt.minute / 60\n",
    "    df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "\n",
    "    # Seasonal yearly component (long wave)\n",
    "    days = (df['timestamp'] - df['timestamp'].min()).dt.total_seconds() / (3600 * 24)\n",
    "    yearly = 10 * np.sin(2 * np.pi * days / 365.25)  # scale\n",
    "\n",
    "    # Weekly cycle\n",
    "    weekly = 2.5 * np.sin(2 * np.pi * days / 7)\n",
    "\n",
    "    # Daily cycle (strong for light)\n",
    "    daily = 30 * np.sin(2 * np.pi * df['hour'] / 24 - 0.5)  # phase shift\n",
    "\n",
    "    # Light (umol m^-2 s^-1): baseline + daily + seasonal + noise\n",
    "    df['Light'] = np.clip(200 + daily + yearly * 5 + np.random.normal(0, 15, n_rows), 50, 800)\n",
    "\n",
    "    # Temperature (C): controlled but with small diurnal and seasonal shifts\n",
    "    df['Temp'] = np.clip(20 + 2 * np.sin(2 * np.pi * df['hour'] / 24) + 1.5 * yearly / 10 + np.random.normal(0, 0.8, n_rows), 15, 28)\n",
    "\n",
    "    # Humidity (%): inverse of temperature to some extent, plus noise and weekly patterns\n",
    "    df['Humidity'] = np.clip(60 - 4 * np.sin(2 * np.pi * df['hour'] / 24) + 3 * np.cos(2 * np.pi * df['dayofweek'] / 7) + np.random.normal(0, 4, n_rows), 30, 95)\n",
    "\n",
    "    # CO2 (ppm): controlled enrichment cycles with periodic boosts\n",
    "    df['CO2'] = np.clip(400 + 60 * (0.5 + 0.5 * np.sign(np.sin(2 * np.pi * df['hour'] / 24))) + 10 * weekly + np.random.normal(0, 15, n_rows), 350, 1200)\n",
    "\n",
    "    # EC (mS/cm): nutrient concentration with mild drift and offsets\n",
    "    df['EC'] = np.clip(1.8 + 0.2 * np.sin(days / 30) + np.random.normal(0, 0.05, n_rows), 1.2, 2.6)\n",
    "\n",
    "    # pH: kept near neutral with small fluctuations\n",
    "    df['pH'] = np.clip(5.8 + 0.2 * np.sin(days / 90) + np.random.normal(0, 0.05, n_rows), 5.4, 6.6)\n",
    "\n",
    "    # Plant Age (days since seeding), cyclic per harvest cycle (e.g., 28-day cycles) with some variation\n",
    "    cycle_length = 28\n",
    "    df['Plant_Age'] = ((days % cycle_length) + np.random.normal(0, 1, n_rows)).clip(0, cycle_length)\n",
    "\n",
    "    # Growth Stage categorical from Plant_Age\n",
    "    bins = [0, 7, 14, 21, 28]\n",
    "    labels = ['G0', 'G1', 'G2', 'G3']\n",
    "    df['Growth_Stage'] = pd.cut(df['Plant_Age'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "    # Base yield influenced by age and light, temperature near optimal ~21Â°C is best\n",
    "    temp_effect = -0.5 * (df['Temp'] - 21) ** 2 + 4  # parabolic peak at 21C\n",
    "    light_effect = 0.01 * df['Light']\n",
    "    co2_effect = 0.003 * (df['CO2'] - 400)\n",
    "    ec_effect = 1.5 * (df['EC'] - 1.8)\n",
    "    ph_effect = -0.8 * (df['pH'] - 5.9) ** 2 + 0.5\n",
    "\n",
    "    # Build target: future harvest weight per plant (g) with noise\n",
    "    base_yield = (0.5 * df['Plant_Age'] + temp_effect + light_effect + co2_effect + ec_effect + ph_effect)\n",
    "    noise = np.random.normal(0, 3, n_rows)\n",
    "\n",
    "    # Add seasonal yield improvement and occasional bump events (management improvements)\n",
    "    management_trend = 0.02 * days\n",
    "    bump = np.where(np.random.rand(n_rows) < 0.005, np.random.uniform(5, 15, n_rows), 0)\n",
    "\n",
    "    df['Yield_g'] = (np.clip(base_yield + management_trend + bump + noise, 5, 150)).round(2)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Generate dataset\n",
    "df = simulate_vertical_farm_data()\n",
    "\n",
    "# Basic sanity checks\n",
    "print(f\"Rows: {len(df)} | Date Range: {df['timestamp'].min().date()} -> {df['timestamp'].max().date()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf9b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data check and descriptive plots\n",
    "\n",
    "# Convert timestamp to index for plotting convenience\n",
    "df_plot = df.set_index('timestamp').resample('D').mean()\n",
    "\n",
    "# Historical yield trend (daily average)\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(df_plot.index, df_plot['Yield_g'], label='Daily Avg Yield (g)')\n",
    "plt.title('Historical Daily Average Yield (g) â€” Simulated')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Yield (g)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Show distribution of core features\n",
    "df[['Yield_g', 'Light', 'Temp', 'Humidity', 'CO2', 'EC', 'pH', 'Plant_Age']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b663ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: rolling window summaries and time features\n",
    "\n",
    "# We'll compute short-term aggregates that an operator or model would reasonably use\n",
    "rolling_hours = 24  # 24-hour mean roughly\n",
    "\n",
    "# Create rolling stats on a resampled hourly dataframe to keep memory reasonable\n",
    "df_hourly = df.set_index('timestamp').resample('H').mean().interpolate()\n",
    "\n",
    "for col in ['Light', 'Temp', 'Humidity', 'CO2', 'EC', 'pH']:\n",
    "    df_hourly[f'{col}_24h_mean'] = df_hourly[col].rolling(window=rolling_hours, min_periods=1).mean()\n",
    "    df_hourly[f'{col}_24h_std'] = df_hourly[col].rolling(window=rolling_hours, min_periods=1).std().fillna(0)\n",
    "\n",
    "# Bring Plant_Age forward as-is and add time features\n",
    "df_hourly['Plant_Age'] = df_hourly['Plant_Age'].fillna(method='ffill')\n",
    "\n",
    "# Reset index and merge with original (downsample original timestamps to hourly mean)\n",
    "df_features = df_hourly.reset_index()\n",
    "\n",
    "# We'll sample from the hourly features back to our original timestamps (nearest)\n",
    "df_merged = pd.merge_asof(df.sort_values('timestamp'), df_features, on='timestamp', direction='nearest', suffixes=(None, '_hr'))\n",
    "\n",
    "# Derive simple time-based features\n",
    "df_merged['hour'] = df_merged['timestamp'].dt.hour\n",
    "df_merged['dayofweek'] = df_merged['timestamp'].dt.dayofweek\n",
    "\n",
    "# Drop any duplicate columns and confirm size\n",
    "df_merged = df_merged.loc[:, ~df_merged.columns.duplicated()]\n",
    "print('Feature set shape:', df_merged.shape)\n",
    "\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f01caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Modeling: Train/Test split, baseline model, and metrics\n",
    "# ------------------------------\n",
    "\n",
    "# Choose features for modeling (exclude timestamp and categorical raw)\n",
    "feature_cols = [\n",
    "    'Light', 'Temp', 'Humidity', 'CO2', 'EC', 'pH', 'Plant_Age',\n",
    "    'Light_24h_mean', 'Temp_24h_mean', 'Humidity_24h_mean', 'CO2_24h_mean', 'EC_24h_mean', 'pH_24h_mean',\n",
    "    'Light_24h_std', 'Temp_24h_std'\n",
    "]\n",
    "\n",
    "# Create dataset and drop missing\n",
    "model_df = df_merged.dropna(subset=feature_cols + ['Yield_g']).copy()\n",
    "\n",
    "# Time-based split: last 20% as hold-out\n",
    "split_idx = int(len(model_df) * 0.8)\n",
    "train = model_df.iloc[:split_idx]\n",
    "test = model_df.iloc[split_idx:]\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "y_train = train['Yield_g']\n",
    "X_test = test[feature_cols]\n",
    "y_test = test['Yield_g']\n",
    "\n",
    "# Scaling features for linear baseline\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Baseline: Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# XGBoost baseline (quick)\n",
    "xgb_base = xgb.XGBRegressor(n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1, tree_method='hist')\n",
    "xgb_base.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation helper\n",
    "from math import sqrt\n",
    "\n",
    "def evaluate_model(name, y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    print(f\"{name} â€” MAE: {mae:.3f} g | RMSE: {rmse:.3f} g | RÂ²: {r2:.3f} | MAPE: {mape:.2f}%\")\n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'MAPE': mape}\n",
    "\n",
    "# Baseline evaluations\n",
    "rf_preds = rf.predict(X_test)\n",
    "xgb_preds = xgb_base.predict(X_test)\n",
    "\n",
    "rf_metrics = evaluate_model('Random Forest', y_test, rf_preds)\n",
    "xgb_base_metrics = evaluate_model('XGBoost (base)', y_test, xgb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b268d8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Hyperparameter tuning for XGBoost (RandomizedSearchCV)\n",
    "# ------------------------------\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 400],\n",
    "    'max_depth': [3, 4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [1, 2, 4]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', tree_method='hist', random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "# TimeSeriesSplit for CV to respect temporal order\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "\n",
    "rand_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=25,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=tscv,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rand_search.fit(X_train, y_train)\n",
    "print('Best params:', rand_search.best_params_)\n",
    "\n",
    "best_xgb = rand_search.best_estimator_\n",
    "best_preds = best_xgb.predict(X_test)\n",
    "best_metrics = evaluate_model('XGBoost (tuned)', y_test, best_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da6c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Compare metrics and visualize Actual vs Predicted\n",
    "# ------------------------------\n",
    "\n",
    "import pandas as _pd\n",
    "\n",
    "metrics_df = _pd.DataFrame([\n",
    "    {**{'Model': 'Random Forest'}, **rf_metrics},\n",
    "    {**{'Model': 'XGBoost (base)'}, **xgb_base_metrics},\n",
    "    {**{'Model': 'XGBoost (tuned)'}, **best_metrics}\n",
    "])\n",
    "metrics_df.set_index('Model', inplace=True)\n",
    "metrics_df.style.format({\"MAE\": \"{:.2f}\", \"RMSE\": \"{:.2f}\", \"R2\": \"{:.3f}\", \"MAPE\": \"{:.2f}%\"})\n",
    "\n",
    "# Actual vs Predicted scatter for tuned XGBoost\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(y_test, best_preds, alpha=0.4)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual Yield (g)')\n",
    "plt.ylabel('Predicted Yield (g)')\n",
    "plt.title(f'Actual vs Predicted â€” XGBoost (RÂ² = {best_metrics[\"R2\"]:.3f})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87397319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Feature importance (bar chart) and SHAP explainability\n",
    "# ------------------------------\n",
    "\n",
    "# Feature importance from XGBoost\n",
    "import numpy as _np\n",
    "fi = best_xgb.feature_importances_\n",
    "fi_df = pd.DataFrame({'feature': feature_cols, 'importance': fi}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Top 8 features\n",
    "top_n = fi_df.head(8)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=top_n, x='importance', y='feature', palette='viridis')\n",
    "plt.title('Top 8 Feature Importances â€” XGBoost')\n",
    "plt.xlabel('Importance')\n",
    "plt.show()\n",
    "\n",
    "# SHAP explanation (TreeExplainer is efficient for XGBoost)\n",
    "print('Computing SHAP values (may take a moment)...')\n",
    "explainer = shap.TreeExplainer(best_xgb)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# SHAP summary plot\n",
    "shap.initjs()\n",
    "shap.summary_plot(shap_values, X_train, plot_type='bar', show=True, max_display=12)\n",
    "\n",
    "# More detailed SHAP summary\n",
    "shap.summary_plot(shap_values, X_train, show=True, max_display=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a18a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Business-friendly interpretation of evaluation metrics\n",
    "# ------------------------------\n",
    "\n",
    "print('Metric interpretations:')\n",
    "print('- MAE (Mean Absolute Error): average error in grams (e.g., MAE=3g means predictions off by ~3g on average)')\n",
    "print('- RMSE (Root Mean Squared Error): penalizes larger mistakes; useful when large errors are costly')\n",
    "print('- RÂ²: proportion of variance explained; closer to 1 is better')\n",
    "print('- MAPE: percentage error (e.g., MAPE=8% â†’ predictions typically within Â±8%)')\n",
    "\n",
    "print('\\nModel summary:')\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a2597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Save model and scaler for production\n",
    "# ------------------------------\n",
    "\n",
    "model_path = 'xgb_yield_model.joblib'\n",
    "scaler_path = 'scaler.joblib'\n",
    "\n",
    "# Save\n",
    "joblib.dump(best_xgb, model_path)\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print('Saved model to', model_path)\n",
    "print('Saved scaler to', scaler_path)\n",
    "\n",
    "# Example: how to make a real-time prediction on a new (single) sensor reading\n",
    "sample = X_test.iloc[0].to_frame().T  # single sample\n",
    "print('\\nSample input:')\n",
    "print(sample)\n",
    "\n",
    "# Predict with model (no scaler required because XGBoost used original feature space)\n",
    "pred = best_xgb.predict(sample)\n",
    "print(f'Predicted yield (g) for sample: {pred[0]:.2f} g')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
