{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92cf3708",
   "metadata": {},
   "source": [
    "# ðŸ’¦ CNN for Sensor-Based Irrigation in Vertical Farms\n",
    "\n",
    "This notebook demonstrates a \"Golden Path\" for automated irrigation using computer vision and sensor fusion. We'll simulate camera-like images and moisture sensor readings, train a CNN to classify moisture stress (dry/normal/wet), explain model decisions with Grad-CAM, compare to simple baselines, and export a TFLite-ready model for edge deployment on devices like a Raspberry Pi.\n",
    "\n",
    "Why CNNs? Convolutional Neural Networks extract spatial features (edges, textures, wilting patterns) using convolutional filters and pooling, making them robust to variable lighting and partial occlusion â€” they outperform static threshold rules in complex, real-world conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb027f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install -q tensorflow scikit-learn matplotlib seaborn joblib opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e911a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports and reproducible settings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import joblib\n",
    "import random\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76665b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Data Simulation: synthetic images + sensor readings\n",
    "# ------------------------------\n",
    "\n",
    "IMG_H, IMG_W, IMG_C = 32, 32, 3\n",
    "N_IMAGES = 5000\n",
    "classes = ['dry', 'normal', 'wet']\n",
    "\n",
    "def make_plant_image(label):\n",
    "    # start with green background\n",
    "    img = np.ones((IMG_H, IMG_W, IMG_C), dtype=np.float32)\n",
    "    base_green = np.array([34, 139, 34]) / 255.0  # leaf-like\n",
    "    img *= base_green\n",
    "\n",
    "    # add texture noise\n",
    "    noise = np.random.normal(0, 0.03, img.shape)\n",
    "    img = np.clip(img + noise, 0, 1)\n",
    "\n",
    "    # modify by label\n",
    "    if label == 'dry':\n",
    "        # desaturate and add brown patches\n",
    "        img *= 0.6\n",
    "        for _ in range(np.random.randint(1, 5)):\n",
    "            x = np.random.randint(0, IMG_W - 4)\n",
    "            y = np.random.randint(0, IMG_H - 4)\n",
    "            w = np.random.randint(2, 6)\n",
    "            h = np.random.randint(2, 6)\n",
    "            patch = np.random.uniform(0.3, 0.6, (h, w, 1)) * np.array([139, 69, 19]) / 255.0\n",
    "            img[y:y+h, x:x+w, :] = np.clip(img[y:y+h, x:x+w, :] * 0.4 + patch * 0.6, 0, 1)\n",
    "    elif label == 'wet':\n",
    "        # slightly darker but more saturated\n",
    "        img *= 0.9\n",
    "        img[:, :, 1] = np.clip(img[:, :, 1] * 1.1 + 0.05, 0, 1)\n",
    "        # brighten center (water sheen)\n",
    "        yy, xx = np.mgrid[0:IMG_H, 0:IMG_W]\n",
    "        cx, cy = IMG_W // 2 + np.random.randint(-3, 3), IMG_H // 2 + np.random.randint(-3, 3)\n",
    "        mask = np.exp(-((xx-cx)**2 + (yy-cy)**2) / (2*4.0))[:,:,None]\n",
    "        img = np.clip(img + 0.05 * mask, 0, 1)\n",
    "    else:\n",
    "        # normal: balanced greens\n",
    "        img *= 0.95\n",
    "\n",
    "    # add random shadow to simulate lighting variation\n",
    "    if np.random.rand() < 0.3:\n",
    "        x = np.random.randint(0, IMG_W - 8)\n",
    "        y = np.random.randint(0, IMG_H - 8)\n",
    "        w = np.random.randint(4, 12)\n",
    "        h = np.random.randint(4, 12)\n",
    "        img[y:y+h, x:x+w, :] *= np.random.uniform(0.5, 0.85)\n",
    "\n",
    "    return (img * 255).astype(np.uint8)\n",
    "\n",
    "# generate\n",
    "X_imgs = np.zeros((N_IMAGES, IMG_H, IMG_W, IMG_C), dtype=np.uint8)\n",
    "y = np.zeros((N_IMAGES,), dtype=np.int32)\n",
    "sensor_readings = np.zeros((N_IMAGES, 2), dtype=np.float32)  # moisture, humidity\n",
    "\n",
    "for i in range(N_IMAGES):\n",
    "    label_idx = np.random.choice(3, p=[0.25, 0.5, 0.25])\n",
    "    label = classes[label_idx]\n",
    "    img = make_plant_image(label)\n",
    "    X_imgs[i] = img\n",
    "    y[i] = label_idx\n",
    "\n",
    "    # sensor readings consistent with label\n",
    "    if label == 'dry':\n",
    "        sensor_readings[i, 0] = np.random.uniform(0.05, 0.25)  # moisture\n",
    "        sensor_readings[i, 1] = np.random.uniform(30, 50)  # humidity\n",
    "    elif label == 'normal':\n",
    "        sensor_readings[i, 0] = np.random.uniform(0.3, 0.6)\n",
    "        sensor_readings[i, 1] = np.random.uniform(50, 70)\n",
    "    else:\n",
    "        sensor_readings[i, 0] = np.random.uniform(0.65, 0.95)\n",
    "        sensor_readings[i, 1] = np.random.uniform(60, 90)\n",
    "\n",
    "print('Images shape:', X_imgs.shape, 'Labels distribution:', np.bincount(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf76ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images and class balance\n",
    "fig, axes = plt.subplots(3, 6, figsize=(12, 6))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    idx = np.where(y == (i % 3))[0][np.random.randint(0, 10)]\n",
    "    ax.imshow(X_imgs[idx])\n",
    "    ax.axis('off')\n",
    "    ax.set_title(classes[y[idx]])\n",
    "plt.suptitle('Sample images (dry / normal / wet examples)')\n",
    "plt.show()\n",
    "\n",
    "# show sensor distribution\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(sensor_readings[:, 0], sensor_readings[:, 1], c=y, cmap='viridis', alpha=0.6)\n",
    "plt.xlabel('Soil moisture (sim)')\n",
    "plt.ylabel('Humidity (%)')\n",
    "plt.title('Sensor reading distribution by class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57160143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Train / Validation / Test split and preprocessing\n",
    "# ------------------------------\n",
    "\n",
    "# Normalize images to [0,1]\n",
    "X = X_imgs.astype('float32') / 255.0\n",
    "\n",
    "# Combine with sensor features for multimodal baseline (optional)\n",
    "X_sensors = sensor_readings.copy()\n",
    "\n",
    "# Split\n",
    "X_train_img, X_test_img, y_train, y_test, X_train_sens, X_test_sens = train_test_split(\n",
    "    X, y, X_sensors, test_size=0.2, random_state=SEED, stratify=y)\n",
    "X_val_img, X_test_img, y_val, y_test, X_val_sens, X_test_sens = train_test_split(\n",
    "    X_test_img, y_test, X_test_sens, test_size=0.5, random_state=SEED, stratify=y_test)\n",
    "\n",
    "print('Train/Val/Test image shapes:', X_train_img.shape, X_val_img.shape, X_test_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c8ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Baseline: threshold rule and simple MLP on sensor features\n",
    "# ------------------------------\n",
    "\n",
    "# Threshold baseline: simple rule on moisture sensor\n",
    "def threshold_rule(sensor):\n",
    "    moisture = sensor[0]\n",
    "    if moisture < 0.28:\n",
    "        return 0  # dry\n",
    "    elif moisture < 0.65:\n",
    "        return 1  # normal\n",
    "    else:\n",
    "        return 2  # wet\n",
    "\n",
    "# evaluate threshold on test set\n",
    "threshold_preds = np.array([threshold_rule(s) for s in X_test_sens])\n",
    "print('Threshold rule accuracy:', accuracy_score(y_test, threshold_preds))\n",
    "\n",
    "# MLP baseline on sensor features\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "mlp = Sequential([Dense(32, activation='relu', input_shape=(X_train_sens.shape[1],)), Dense(16, activation='relu'), Dense(3, activation='softmax')])\n",
    "mlp.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "mlp.fit(X_train_sens, y_train, validation_data=(X_val_sens, y_val), epochs=20, batch_size=64, verbose=1)\n",
    "\n",
    "mlp_preds = np.argmax(mlp.predict(X_test_sens), axis=1)\n",
    "print('MLP accuracy:', accuracy_score(y_test, mlp_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec5dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# CNN model: Conv -> Pool -> Conv -> Dense\n",
    "# ------------------------------\n",
    "\n",
    "def build_cnn(input_shape=(IMG_H, IMG_W, IMG_C)):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "cnn = build_cnn()\n",
    "cnn.summary()\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_cnn.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "early = EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True)\n",
    "\n",
    "history = cnn.fit(X_train_img, y_train, validation_data=(X_val_img, y_val), epochs=40, batch_size=64, callbacks=[checkpoint, early], verbose=2)\n",
    "\n",
    "# Plot accuracy and loss\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'], label='train_acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "plt.title('CNN Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760b5d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Evaluation: confusion matrix, classification report, and Water Efficiency Score\n",
    "# ------------------------------\n",
    "# Predict\n",
    "cnn_preds = np.argmax(cnn.predict(X_test_img), axis=1)\n",
    "\n",
    "print('CNN accuracy:', accuracy_score(y_test, cnn_preds))\n",
    "print('\\nClassification Report:\\n', classification_report(y_test, cnn_preds, target_names=classes))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, cnn_preds)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix â€” CNN')\n",
    "plt.show()\n",
    "\n",
    "# Water Efficiency Score (business metric): lower false positives for wet reduce wasted watering\n",
    "# Define: WES = 100 * (1 - (FP_wet + FN_dry) / N)\n",
    "FP_wet = np.sum((y_test != 2) & (cnn_preds == 2))\n",
    "FN_dry = np.sum((y_test == 0) & (cnn_preds != 0))\n",
    "wes = 100 * (1 - (FP_wet + FN_dry) / len(y_test))\n",
    "print(f'Water Efficiency Score (WES): {wes:.2f} (higher is better)')\n",
    "\n",
    "# Compare MLP and threshold\n",
    "print('MLP accuracy:', accuracy_score(y_test, mlp_preds))\n",
    "print('Threshold accuracy:', accuracy_score(y_test, threshold_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8026ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Grad-CAM: simple implementation for Keras models\n",
    "# ------------------------------\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(predictions[0])\n",
    "        class_channel = predictions[:, pred_index]\n",
    "    grads = tape.gradient(class_channel, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "    heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + 1e-9)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "# Pick some test images and show Grad-CAM\n",
    "last_conv_layer = 'conv2d_1'  # name may depend on model summary\n",
    "indices = np.random.choice(len(X_test_img), size=6, replace=False)\n",
    "fig, axes = plt.subplots(2, 6, figsize=(16, 6))\n",
    "for i, idx in enumerate(indices):\n",
    "    img = X_test_img[idx:idx+1]\n",
    "    heatmap = make_gradcam_heatmap(img, cnn, last_conv_layer)\n",
    "    heatmap = cv2.resize(heatmap, (IMG_W, IMG_H))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    overlay = cv2.addWeighted((img[0]*255).astype('uint8'), 0.6, heatmap_color, 0.4, 0)\n",
    "\n",
    "    ax_img = axes[0, i]\n",
    "    ax_img.imshow(img[0])\n",
    "    ax_img.axis('off')\n",
    "    ax_img.set_title(f'True: {classes[y_test[idx]]}\\nPred: {classes[cnn_preds[idx]]}')\n",
    "\n",
    "    ax_overlay = axes[1, i]\n",
    "    ax_overlay.imshow(overlay)\n",
    "    ax_overlay.axis('off')\n",
    "plt.suptitle('Grad-CAM overlays (true vs predicted)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c1d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Sample Image Grid: Actual vs Predicted\n",
    "# ------------------------------\n",
    "\n",
    "n = 9\n",
    "idxs = np.random.choice(len(X_test_img), size=n, replace=False)\n",
    "fig, ax = plt.subplots(3, 3, figsize=(8, 8))\n",
    "for i, idx in enumerate(idxs):\n",
    "    r, c = divmod(i, 3)\n",
    "    ax[r, c].imshow(X_test_img[idx])\n",
    "    ax[r, c].axis('off')\n",
    "    ax[r, c].set_title(f'T:{classes[y_test[idx]]} / P:{classes[cnn_preds[idx]]}')\n",
    "plt.suptitle('Sample: Actual vs Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1d3f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Hybrid: brief moisture trend forecasting with LSTM (optional)\n",
    "# ------------------------------\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# simulate moisture time-series and build sequences\n",
    "moisture = np.clip(0.5 + 0.2 * np.sin(np.linspace(0, 50, 10000)) + np.random.normal(0, 0.05, 10000), 0, 1)\n",
    "window = 24\n",
    "X_seq = np.array([moisture[i-window:i] for i in range(window, len(moisture))])\n",
    "y_seq = moisture[window:]\n",
    "\n",
    "# train/test split\n",
    "split = int(0.8 * len(X_seq))\n",
    "X_train_seq = X_seq[:split]\n",
    "y_train_seq = y_seq[:split]\n",
    "X_test_seq = X_seq[split:split+500]\n",
    "y_test_seq = y_seq[split:split+500]\n",
    "\n",
    "# reshape for LSTM\n",
    "X_train_seq = X_train_seq[..., np.newaxis]\n",
    "X_test_seq = X_test_seq[..., np.newaxis]\n",
    "\n",
    "lstm = Sequential([LSTM(32, input_shape=(window, 1)), Dense(16, activation='relu'), Dense(1)])\n",
    "lstm.compile(optimizer='adam', loss='mse')\n",
    "lstm.fit(X_train_seq, y_train_seq, epochs=8, batch_size=256, verbose=1)\n",
    "\n",
    "pred_seq = lstm.predict(X_test_seq)\n",
    "plt.figure()\n",
    "plt.plot(y_test_seq[:200], label='True moisture')\n",
    "plt.plot(pred_seq[:200], label='Predicted moisture')\n",
    "plt.title('Moisture trend forecasting (LSTM)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b00e9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Export: save model and convert to TFLite for edge deployment\n",
    "# ------------------------------\n",
    "cnn.save('irrigation_cnn.h5')\n",
    "print('Saved CNN to irrigation_cnn.h5')\n",
    "\n",
    "try:\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(cnn)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "    with open('irrigation_cnn.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    print('Saved TFLite model to irrigation_cnn.tflite')\n",
    "except Exception as e:\n",
    "    print('TFLite conversion skipped (requires TF full runtime):', e)\n",
    "\n",
    "# Sample inference snippet for edge device\n",
    "print('\\nSample inference (pseudo):')\n",
    "print(\"# img = capture_from_camera(); img = preprocess(img); preds = model.predict(img[None,...]); action = np.argmax(preds)\")\n",
    "\n",
    "# Deployment checklist\n",
    "print('\\nDeployment checklist:')\n",
    "print('- Quantize model and test on Raspberry Pi / Coral for latency')\n",
    "print('- Implement zone-specific actuation (map detection to valve/pump control)')\n",
    "print('- Add failsafe thresholds and human-in-loop overrides')\n",
    "print('- Log decisions and sensor readings for drift detection and retraining')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe1a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Irrigation Insights for Operators\n",
    "# ------------------------------\n",
    "print('Irrigation insights:')\n",
    "print('- Use Grad-CAM overlays to identify dry subzones and trigger zone-specific drip to save ~20% water.')\n",
    "print('- Combine camera predictions with moisture sensor thresholds for high-confidence actuation (ensemble rule).')\n",
    "print('- Track WES over time to quantify water savings and tune false positive/negative costs per crop type.')\n",
    "\n",
    "print('\\nBusiness-friendly interpretation:')\n",
    "print('- Accuracy / F1 relate directly to water waste and crop stress: an increase in F1 by 0.1 can map to 10-25% water savings in typical scenarios (estimate; validate on-farm).')\n",
    "print('- Visual explainability (Grad-CAM) reduces operator trust barriers and enables targeted interventions.')\n",
    "\n",
    "print('\\nNotebook complete â€” Golden Path ready for experiment and edge deployment.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
